{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks in Python: From Sklearn to PyTorch and Probabilistic Neural Networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial covers different concepts related to neural networks with Sklearn and PyTorch. Neural networks have gained lots of attention in machine learning (ML) in the past decade with the development of deeper network architectures (known as deep learning). These models have even surpassed human capabilities in different vision and natural language processing datasets. For example a neural network trained on the well-known ImageNet object recognition database tells the difference between different breeds of dog with an error rate of just 4.58%. For comparison, the typical human gets around 5%. Read more about this here: \n",
    "https://www.theguardian.com/global/2015/may/13/baidu-minwa-supercomputer-better-than-humans-recognising-images\n",
    "\n",
    "In this tutorial, we will first see how easy it is to train multilayer perceptrons in Sklearn with the well-known handwritten dataset MNIST. Things will then get a bit more advanced with PyTorch. We will first train a network with four layers (deeper than the one we will use with Sklearn) to learn with the same dataset and then see a little bit on Bayesian (probabilistic) neural networks. This tutorial assumes some basic knowledge of python and neural networks.\n",
    "\n",
    "## What exactly are scikit-learn and PyTorch?\n",
    "\n",
    "* Scikit-learn is a free software machine learning library for Python which makes unbelievably easy to train traditional ML models such as Support Vector Machines or Multilayer Perceptrons. \n",
    "\n",
    "* PyTorch is an open source machine learning library based on Torch, used for coding deep learning algorithms and primarily developed by Facebook's artificial intelligence research group. \n",
    "\n",
    "## ...and why should I care about Pytorch and Probabilistic Neural Networks?\n",
    "\n",
    "1. **Many people prefer PyTorch to TensorFlow**. This is mainly because PyTorch allows for dynamic computational graphs (meaning that you can change the network architecture during running time, which is quite useful for certain neural network architectures) and it's very easy to learn (building ML models is actually very intuitive, as we will see). \n",
    "\n",
    "2. **ML needs to account for uncertainty!** Have you heard of probabilistic programming? It's a programming paradigm in which you can easily specify probabilistic models and perform inference on them. These languages greatly simplify the task of creating systems that handle uncertainty. For example, Pyro (from Uber AI Labs) enables flexible and expressive deep probabilistic modeling, unifying the best of modern deep learning and Bayesian modeling. And while we won't touch on probabilistic programming in this tutorial, you may want to know why probabilistic approaches are so needed in ML and why these languages are growing so quickly. The network we are going to build doesn't use probabilistic programming languages, but it's still probabilistic!\n",
    "\n",
    "\n",
    "Anyway... let's get to it, shall we? \n",
    "\n",
    "Let's start with Sklearn and then we will move to PyTorch and finally include some notions of PNNs into the equation..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer Perceptron in Sklearn to classify handwritten digits\n",
    "The dataset we are going to use (MNIST) is still one of the most used benchmarks in computer vision tasks, where one needs to go from an image of a handwritten digit to the digit itself (0, 1, 2...). This could be done with a Convolutional Neural Network, which are the state-of-the-art method for discovering spatial patterns. However, to simplify this tutorial what we will do is to unroll/flatten the image into a vector (images are 28x28 pixels, which will result in a vector of size 784, where each element represents a pixel) and use a fully connected neural network. \n",
    "\n",
    "What we are aiming to do is to build a mathematical function that can predict the characteristic of interest (digit) based on the pixels. This is where neural networks come in handy, as they are mathematical functions that are universal approximators (can approximate any function given enough degrees of freedom). Neural networks implement linear functions. However, they can also include nonlinear transformations known as activation units (for example a logistic function), which allows them to provide non-linear decision regions! \n",
    "\n",
    "Let's see how easy it would be to do so in Sklearn... We will build both a simple linear perceptron and a multilayer perceptron with the default activation functions in Sklearn, which are the so-called ReLU. When you run the code don't forget to compare the accuracy of both models and play around with the hyperparameters and network architecture!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron: Accuracy in train is 0.90\n",
      "Perceptron: Accuracy in test is 0.88\n",
      "Iteration 1, loss = 0.78470269\n",
      "Iteration 2, loss = 0.40328050\n",
      "Iteration 3, loss = 0.35136023\n",
      "Iteration 4, loss = 0.20348133\n",
      "Iteration 5, loss = 0.16570566\n",
      "Iteration 6, loss = 0.13211615\n",
      "Iteration 7, loss = 0.11207030\n",
      "Iteration 8, loss = 0.09866746\n",
      "Iteration 9, loss = 0.09422566\n",
      "Iteration 10, loss = 0.08217377\n",
      "Iteration 11, loss = 0.07991503\n",
      "Iteration 12, loss = 0.06608675\n",
      "Iteration 13, loss = 0.06453361\n",
      "Iteration 14, loss = 0.05468683\n",
      "Iteration 15, loss = 0.04876809\n",
      "Iteration 16, loss = 0.04297760\n",
      "Iteration 17, loss = 0.03573632\n",
      "Iteration 18, loss = 0.03611793\n",
      "Iteration 19, loss = 0.04342410\n",
      "Iteration 20, loss = 0.03933435\n",
      "Iteration 21, loss = 0.03636544\n",
      "Iteration 22, loss = 0.03213834\n",
      "Iteration 23, loss = 0.04205857\n",
      "Iteration 24, loss = 0.03283863\n",
      "Iteration 25, loss = 0.04721034\n",
      "Iteration 26, loss = 0.04447085\n",
      "Iteration 27, loss = 0.02717074\n",
      "Iteration 28, loss = 0.01740198\n",
      "Iteration 29, loss = 0.02275094\n",
      "Iteration 30, loss = 0.01959126\n",
      "Iteration 31, loss = 0.02962673\n",
      "Iteration 32, loss = 0.03263101\n",
      "Iteration 33, loss = 0.03082928\n",
      "Iteration 34, loss = 0.02403898\n",
      "Iteration 35, loss = 0.02040798\n",
      "Iteration 36, loss = 0.01918966\n",
      "Iteration 37, loss = 0.01850323\n",
      "Iteration 38, loss = 0.01064574\n",
      "Iteration 39, loss = 0.01135439\n",
      "Iteration 40, loss = 0.02319037\n",
      "Iteration 41, loss = 0.02409221\n",
      "Iteration 42, loss = 0.02342306\n",
      "Iteration 43, loss = 0.01794512\n",
      "Iteration 44, loss = 0.01683873\n",
      "Iteration 45, loss = 0.01382962\n",
      "Iteration 46, loss = 0.01547388\n",
      "Iteration 47, loss = 0.01173309\n",
      "Iteration 48, loss = 0.01763442\n",
      "Iteration 49, loss = 0.02567021\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Multilayer Perceptron: Accuracy in train is 1.00\n",
      "Multilayer Perceptron: Accuracy in test is 0.97\n"
     ]
    }
   ],
   "source": [
    "# we import the necessary libraries\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# load and partition MNIST dataset\n",
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
    "# you can check how if we change random_state (seed for test/train split)\n",
    "# the accuracy of our models also change!\n",
    "X_train, X_test, y_train, y_test = train_test_split(X/255., y, test_size = 0.20, random_state=1)\n",
    "\n",
    "# first let's use a very simple linear perceptron: we set the hyperparams and train\n",
    "# a perceptron does not use activation units which means it's a completely linear model\n",
    "per = Perceptron(random_state=1, max_iter=30, tol=0.001)\n",
    "per.fit(X_train, y_train)\n",
    "\n",
    "# we predict with our built perceptron\n",
    "yhat_train_per = per.predict(X_train)\n",
    "yhat_test_per = per.predict(X_test)\n",
    "\n",
    "print(f\"Perceptron: Accuracy in train is %.2f\" % (accuracy_score(y_train, yhat_train_per)))\n",
    "print(f\"Perceptron: Accuracy in test is %.2f\" % (accuracy_score(y_test, yhat_test_per)))\n",
    "\n",
    "# now let's try a multilayer perceptron\n",
    "# the default activation function is ReLU\n",
    "mlp = MLPClassifier(max_iter=50, alpha=1e-4, solver='sgd', verbose=10, random_state=1,\n",
    "                    learning_rate_init=.1, hidden_layer_sizes=(784, 100, 2))\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "# predict with our new classifier\n",
    "yhat_train_mlp = mlp.predict(X_train)\n",
    "yhat_test_mlp = mlp.predict(X_test)\n",
    "\n",
    "print(f\"Multilayer Perceptron: Accuracy in train is %.2f\" % (accuracy_score(y_train, yhat_train_mlp)))\n",
    "print(f\"Multilayer Perceptron: Accuracy in test is %.2f\" % (accuracy_score(y_test, yhat_test_mlp)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A standard Neural Network in PyTorch to classify MNIST\n",
    "The Torch module provides all the necessary tensor operators you will need to build your first neural network in PyTorch. And yes, in PyTorch everything is a Tensor. This is because PyTorch is mostly used for deep learning, as opposed to Sklearn, which implements more traditional and shallower ML models.\n",
    "\n",
    "Now we will use a similar architecture to the one we used before with Sklearn but deeper, this means that it will need to train many more parameters. We could have built exactly this same model in Sklearn, but it would have taken longer to train, as we will be using GPU to train our model in PyTorch. PyTorch would also be helpful when training more complex architectures (such as the previously mentioned Convolutional Neural Networks, which would be the ideal way of handling this computer vision dataset).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you will need the torch and torchvision packages, so don't forget to install them on your machine!\n",
    "# firstly, we import the necessary packages\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then we set some global variables that we will need later on. We are going to\n",
    "# work with MNIST(the famous handwritten digit classification dataset, in which our\n",
    "# data comes in the form of images)\n",
    "\n",
    "# division of the dataset in batches of 100 data points to be able to fit them in memory\n",
    "BATCH_SIZE = 100\n",
    "# 1 epoch = training with all batches in the dataset\n",
    "TRAIN_EPOCHS = 10\n",
    "# number of classes in the dataset\n",
    "CLASSES = 10\n",
    "# size in pixels of our images, in this case 28 pixels height and 28 pixels width\n",
    "INPUT_HEIGHT = 28\n",
    "INPUT_WIDTH = 28\n",
    "# TOTAL_INPUT = INPUT_HEIGHT * INPUT_WIDTH\n",
    "TOTAL_INPUT = 784\n",
    "# number of cases for train and test\n",
    "TRAIN_SIZE = 50000\n",
    "TEST_SIZE = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we define our model through a class since it's the recommended way to build the\n",
    "# computational graph (just think of this graph as a nice and clean way to think about \n",
    "# mathematical expressions). The class header contains the name of the class FCN \n",
    "# (Fully Connected Network) and the parameter nn.Module which basically indicates that\n",
    "# we are defining a customized neural network.\n",
    "\n",
    "class FCN(nn.Module):\n",
    "    # FCN is the class for my fully connected network, it inherits from nn.Module, which is the base\n",
    "    # class for all neural network modules in Torch.\n",
    "\n",
    "    # The next step is to define the initializations that will be performed upon creating an instance\n",
    "    # of the customized neural network. \n",
    "    def __init__(self, n_hidden=600, n_classes=CLASSES):\n",
    "        super().__init__()\n",
    "        # all we do is to define our model, which is going to have four layers (l1 to l4). \n",
    "        # The number of neurons in hidden layers is n_hidden. nn.linear includes weights and biases.\n",
    "        self.l1 = nn.Linear(INPUT_HEIGHT*INPUT_WIDTH, n_hidden)\n",
    "        self.l2 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.l3 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.l4 = nn.Linear(n_hidden, n_classes)\n",
    "\n",
    "    # This function is where the magic happens. This is where the data enters and is fed\n",
    "    # into the computational graph (i.e., the neural network structure we have built). \n",
    "    def forward(self, x):\n",
    "        # propagates forward the data to the output neuron\n",
    "        x = x.view(-1, INPUT_HEIGHT*INPUT_WIDTH)\n",
    "        x = F.relu(self.l1(x))\n",
    "        x = F.relu(self.l2(x))\n",
    "        x = F.relu(self.l3(x))\n",
    "        x = F.log_softmax(self.l4(x), dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we are going to define two independent functions to train and test the network with our data\n",
    "# we will use them later in the main part of the code\n",
    "def train_fcn(net, optimizer, epoch, train_loader):\n",
    "    # trains the network with one batch at a time\n",
    "    net.train()\n",
    "    total_loss = 0.0\n",
    "    total = 0.0\n",
    "    correct = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "        # sets gradients of all model parameters to zero\n",
    "        net.zero_grad()\n",
    "        # propagate inputs forward to get a prediction\n",
    "        pred = net.forward(images.cuda().view(-1, 784))\n",
    "        # compute loss and stats of performance, we transform the classes using a hot labels\n",
    "        # encoding (which is a common approach to solve multiclass problems) and use binary cross entropy\n",
    "        loss = F.binary_cross_entropy_with_logits(\n",
    "            pred, F.one_hot(labels.cuda(), CLASSES).float())\n",
    "        total_loss += loss\n",
    "        total += labels.size(0)\n",
    "        correct += (pred.argmax(-1) == labels.cuda()).sum().item()\n",
    "        # propagate backward\n",
    "        loss.backward()\n",
    "        # optimise parameters of the network\n",
    "        optimizer.step()\n",
    "    # print performance for this epoch\n",
    "    print(\n",
    "        f\"Epoch {epoch}: loss {total_loss:.5f} accuracy {correct / total * 100:.5f}\")\n",
    "\n",
    "def test_fcn(net, test_loader):\n",
    "    # computes test accuracy\n",
    "    total = 0.0\n",
    "    correct = 0.0\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "        pred = net.forward(images.cuda().view(-1, 784))\n",
    "        total += labels.size(0)\n",
    "        correct += (pred.argmax(-1) == labels.cuda()).sum().item()\n",
    "    print(f\"Test accuracy: {correct / total * 100:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch 0: loss 63.53376 accuracy 83.78600\n",
      "Epoch 1: loss 43.67813 accuracy 94.97800\n",
      "Epoch 2: loss 41.00309 accuracy 96.47600\n",
      "Epoch 3: loss 39.71790 accuracy 97.10400\n",
      "Epoch 4: loss 38.77234 accuracy 97.63000\n",
      "Epoch 5: loss 38.07506 accuracy 98.06800\n",
      "Epoch 6: loss 37.74971 accuracy 98.17000\n",
      "Epoch 7: loss 37.04946 accuracy 98.61000\n",
      "Epoch 8: loss 36.80310 accuracy 98.72800\n",
      "Epoch 9: loss 36.77231 accuracy 98.74800\n",
      "Test accuracy: 97.67000\n"
     ]
    }
   ],
   "source": [
    "# And this is it... where we train and test our network!\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Comment if you want to run on CPU\n",
    "torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "\n",
    "# convert to tensor and normalise data\n",
    "tr = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Set download=True to download the dataset from the internet\n",
    "mnist = torchvision.datasets.mnist.MNIST(\n",
    "    root='images', transform=tr, download=True)\n",
    "\n",
    "# divide data in train and test\n",
    "train_set, test_set = torch.utils.data.random_split(\n",
    "    mnist, lengths=(TRAIN_SIZE, TEST_SIZE))\n",
    "\n",
    "# load the batches\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# create an instance of our neural network\n",
    "fcn = FCN().to(DEVICE)\n",
    "\n",
    "# set the optimiser (Stochastic Gradient Descent) with the hyperparams\n",
    "optim = torch.optim.SGD(fcn.parameters(recurse=True), lr=0.1, momentum=0.95)\n",
    "\n",
    "print(\"Training...\")\n",
    "# train and test!\n",
    "for epoch in range(TRAIN_EPOCHS):\n",
    "    train_fcn(fcn, optim, epoch, train_loader)\n",
    "test_fcn(fcn, test_loader)\n",
    "# ...and we are DONE!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To recap... We defined the class with the architecture of our neural network, a train and test functions and the main part of our code (which was really simple: download data, partition, preprocess, set optimiser and hyperparameters and train and test). We can see that we achieve really good accuracy in test after training for 10 epochs. In this case, we used a network composed of 4 layers with ReLU activation functions, which seems to work really well for our dataset. However, these and other hyperparameters (such as learning rate) need to be optimised in order to find the best configuration for each specific problem. I will leave it to you to play with these hyperparameters and compare the accuracies of our network with for example 3 and 5 layers. \n",
    "\n",
    "Now, let's move to the probabilistic version.\n",
    "\n",
    "## Wait a moment... What is a Probabilistic Neural Network anyway?\n",
    "\n",
    "Bayesian neural networks (from now on BNNs) use the Bayes rule to create a probabilistic neural network. BNNs can be defined as feedforward neural networks that include *notions of uncertainty in their parameters*. Let us go back for a moment to the equation of a simple linear perceptron: y = W*X + b, where X is our input data and y our targets. The parameters to learn are W and b and these are usually optimised through maximum likelihood estimation. However, instead of having simply one scalar parameter for b (or any of the elements in W), we could learn a distribution (Normal, Laplace, etc.). In the case of a Normal distribution, we will then define each parameter with a mean and standard deviation. This is the idea behind the paper \"Weight Uncertainty in Neural Networks\" from Blundell et. al. at Google: The BNN learns a distribution for every network parameter. \n",
    "\n",
    "This is useful for several reasons: \n",
    "\n",
    "* Firstly, **it allows our network to produce uncertainty outcomes or even say \"I don't know\"**. Why is this important, you may ask? Just imagine that you have a system that distinguishes dogs from cats (what a cliché...), what do you think will happen if you input let's say a picture of yourself? Well, it's going to choose either dog and cat, which initially may sound cool, but let's face it, it's not very useful in practice. So the first advantage of a probabilistic neural network is to be able to say: \"Actually, I'm not sure which class this test data point belongs to!\"\n",
    "\n",
    "* The second cool advantage of BNNs is that they are **easy to prune**. For example, if after training our network we have a weight: 1) which mean is close to zero and 2) we are very sure about it (this is, uncertainty is very low) we can prune the neuron associated to it (easy peasy!). Pruning a model is usually important in real-world applications.\n",
    "\n",
    "* **It regularises the weights**, improving performance and yielding comparable performance to the commonly used dropout technique.\n",
    "\n",
    "On the other hand, not everything is greener on the other side. BNNs tend to be slower than their non-probabilistic analogues at classifying new cases and they require more memory space to store the model.\n",
    "\n",
    "Now... **is building a BNN going to get much more difficult than our previous example?** A bit, but let's clarify some concepts first.\n",
    "\n",
    "1. The first thing you need to understand about these networks is that to test them with a data point we will need to **sample from the distribution of parameters** (or with the expected value otherwise). In practice, these network work then as an ensemble, producing multiple results for the same data point (this coming from the fact that we are sampling several times from the distribution of parameters). These outputs can be then averaged to get the final prediction and a notion of uncertainty in the prediction.\n",
    "\n",
    "2. The second concept is that as the word Bayesian indicates, we impose a prior on the network parameters. We usually define the distribution (e.g. Normal) and we initalise the parameters with the prior. In this case, we use an engineered mixture of Gaussians for the prior.\n",
    "\n",
    "Now, this tutorial will only give you an intuition of how BNNs work. We won't dive much deeper into variational inference, which is the approach these network use to approximate the distribution of learnt parameters. At this point, you just need to know that variational inference is one of the most common approaches (apart from sampling) to approximate a posterior distribution (you may have heard of variational autoencoders before). For more information please refer to the paper \"Weight Uncertainty in Neural Networks\" and get deeper into the theory behind it! The implementation in this paper is inspired by several other implementations of the same idea, specially the one in https://www.nitarshan.com/bayes-by-backprop/.\n",
    "\n",
    "What we need for this code is to define 1. the architecture (number of layers + the definition of a Bayesian layer), 2. a loss function to define how to account for misclassification errors and use during learning and 3. the train and test functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need a few more imports for this part...\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of times we will sample from the distribution of parameters\n",
    "SAMPLES = 10\n",
    "# these are needed for the mixture of Gaussians for the prior\n",
    "PI = 0.5\n",
    "SIGMA1 = torch.cuda.FloatTensor([math.exp(-0)])\n",
    "SIGMA2 = torch.cuda.FloatTensor([math.exp(-6)])\n",
    "NUM_BATCHES = len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our Bayesian NN is going to be similarly defined than our previous class FCN...\n",
    "class BNN(nn.Module):\n",
    "    # As you can see it's almost the same, we just changed nn.linear for BayesianLinear\n",
    "    # which is a type of layer that we will need to define later...\n",
    "    def __init__(self, n_hidden=600, n_classes=CLASSES):\n",
    "        super().__init__()\n",
    "        self.l1 = BayesianLinear(INPUT_HEIGHT*INPUT_WIDTH, n_hidden)\n",
    "        self.l2 = BayesianLinear(n_hidden, n_hidden)\n",
    "        self.l3 = BayesianLinear(n_hidden, n_hidden)\n",
    "        self.l4 = BayesianLinear(n_hidden, n_classes)\n",
    "\n",
    "    # the forward function is also very very similar the main difference is the attribute sample. \n",
    "    # This attribute is needed because we have now a distribution of parameters and everytime we \n",
    "    # propagate forward in our network we need to sample from the distributions of parameters.\n",
    "    # If we don't sample we simply take the expected value (mean)\n",
    "    def forward(self, x, sample=False):\n",
    "        x = x.view(-1, INPUT_HEIGHT*INPUT_WIDTH)\n",
    "        x = F.relu(self.l1(x, sample))\n",
    "        x = F.relu(self.l2(x, sample))\n",
    "        x = F.relu(self.l3(x, sample))\n",
    "        x = F.log_softmax(self.l4(x, sample), dim=1)\n",
    "        return x\n",
    "\n",
    "    # We also need some additional functions. These implement equations needed to perform \n",
    "    # variational inference,which we use to approximate the posterior of the parameters\n",
    "    # (i.e. in practice, learn the distribution of the parameters) .\n",
    "    # The next two functions are defined per layer and the final result is the sum of each layer\n",
    "    def log_prior(self):\n",
    "        return self.l1.log_prior + self.l2.log_prior + self.l3.log_prior + self.l4.log_prior\n",
    "\n",
    "    def log_variational_post(self):\n",
    "        return self.l1.log_variational_post + self.l2.log_variational_post + self.l3.log_variational_post + self.l4.log_variational_post\n",
    "\n",
    "    # ELBO stands for Evidence Lower BOund. This function implements the loss used to learn\n",
    "    # the distribution over the parameters of the network. The equation is obtained through\n",
    "    # the use of variationalinference, which as we said we won't touch that much on.\n",
    "    def elbo(self, input, target, samples=SAMPLES, batch_size=BATCH_SIZE):\n",
    "        outputs = torch.zeros(samples, batch_size, CLASSES)\n",
    "        log_priors = torch.zeros(samples)\n",
    "        log_variational_post = torch.zeros(samples)\n",
    "        # as you can see, here we sample multiple times and we thus have multiple predictions\n",
    "        # for the same data\n",
    "        for i in range(samples):\n",
    "            outputs[i] = self(input, sample=True)\n",
    "            log_priors[i] = self.log_prior()\n",
    "            log_variational_post[i] = self.log_variational_post()\n",
    "        # average log-priors and log-posteriors\n",
    "        log_prior = log_priors.mean()\n",
    "        log_variational_post = log_variational_post.mean()\n",
    "        # we take the mean of the predictions using outputs.mean(0)\n",
    "        # calculate Negative Log Likelihood loss\n",
    "        nll = F.nll_loss(outputs.mean(0), target, reduction='sum')\n",
    "        # calculate KL divergence\n",
    "        kl = (log_variational_post - log_prior) / NUM_BATCHES\n",
    "        # this is the ELBO loss function (defined in the paper)\n",
    "        return kl + nll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ok, but we need to define the BayesianLinear layer!\n",
    "class BayesianLinear(nn.Module):\n",
    "\n",
    "  def __init__(self, dim_in, dim_out):\n",
    "    super(BayesianLinear, self).__init__()\n",
    "    self.dim_in = dim_in\n",
    "    self.dim_out = dim_out\n",
    "\n",
    "    # now in this case since we will assume a Gaussian distribution of our parameters we will need two\n",
    "    # parameters (mean and std), however, in the paper it is shown that it's more efficient to use rho\n",
    "    # which is a transformation of sigma (the std). We thus initialise the means and scales (rho).\n",
    "    self.w_mu = nn.Parameter((-0.2 - 0.2) * torch.rand(dim_out, dim_in) + 0.2)\n",
    "    self.w_rho = nn.Parameter((-5. + 4.) * torch.rand(dim_out, dim_in) - 4.)\n",
    "    # we create a Gaussian distribution for w\n",
    "    self.w = Gaussian(self.w_mu, self.w_rho)\n",
    "\n",
    "    # same is applicable to the biases\n",
    "    self.b_mu = nn.Parameter((-0.2 - 0.2) * torch.rand(dim_out) + 0.2)\n",
    "    self.b_rho = nn.Parameter((-5. + 4.) * torch.rand(dim_out) - 4.)\n",
    "    self.b = Gaussian(self.b_mu, self.b_rho)\n",
    "\n",
    "    # the prior distribution is defined in the paper as an engineered mixture of two Gaussians, where\n",
    "    # PI is the parameter for the mixture\n",
    "    self.w_prior = ScaledGaussianMixture(PI, SIGMA1, SIGMA2)\n",
    "    self.b_prior = ScaledGaussianMixture(PI, SIGMA1, SIGMA2)\n",
    "    self.log_prior = 0\n",
    "    self.log_variational_post = 0 \n",
    "\n",
    "  def forward(self, input, sample=False, calc_log_prob=False):\n",
    "\n",
    "    if self.training or sample:  # while training or sampling\n",
    "      w = self.w.sample()\n",
    "      b = self.b.sample()\n",
    "    else:\n",
    "      w = self.w.mu\n",
    "      b = self.b.mu\n",
    "\n",
    "    if self.training or calc_log_prob:\n",
    "      # calculate logprob of prior for sampled weights\n",
    "      self.log_prior = self.w_prior.log_prob(w) + self.b_prior.log_prob(b)\n",
    "      # calculate logprob of posterior (w, b) distributions\n",
    "      self.log_variational_post = self.w.log_prob(w) + self.b.log_prob(b)\n",
    "    else:\n",
    "      self.log_prior, self.log_variational_post = 0, 0\n",
    "\n",
    "    return F.linear(input, w, b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we now define the type of object that our BayesianLinear function is based on\n",
    "class Gaussian:\n",
    "    def __init__(self, mu, rho):\n",
    "        self.mu = mu\n",
    "        self.rho = rho\n",
    "        self.normal = torch.distributions.Normal(0, 1)\n",
    "\n",
    "    @property\n",
    "    def sigma(self):\n",
    "        # log1p <- ln(1 + input)\n",
    "        return torch.log1p(torch.exp(self.rho))  \n",
    "\n",
    "    # we implement how to sample from a Normal distribution\n",
    "    def sample(self):\n",
    "        epsilon = self.normal.sample(self.mu.size())\n",
    "        return self.mu + self.sigma * epsilon\n",
    "\n",
    "    def log_prob(self, input):\n",
    "        return (-math.log(math.sqrt(2 * math.pi))\n",
    "            - torch.log(self.sigma)\n",
    "            - ((input - self.mu) ** 2) / (2 * self.sigma ** 2)).sum()\n",
    "\n",
    "class ScaledGaussianMixture:\n",
    "    def __init__(self, pi, sigma1, sigma2):\n",
    "        self.pi = pi\n",
    "        self.sigma1 = sigma1\n",
    "        self.sigma2 = sigma2\n",
    "        self.gaussian1 = torch.distributions.Normal(0, sigma1)\n",
    "        self.gaussian2 = torch.distributions.Normal(0, sigma2)\n",
    "\n",
    "    def log_prob(self, input):\n",
    "        prob1 = torch.exp(self.gaussian1.log_prob(input))\n",
    "        prob2 = torch.exp(self.gaussian1.log_prob(input))\n",
    "        return (torch.log(self.pi * prob1 + (1 - self.pi) * prob2)).sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's reimplement our train and test functions\n",
    "def train_bnn(net, optimizer, epoch, train_loader):\n",
    "    # trains the network with one batch at a time\n",
    "    net.train()\n",
    "    total_loss = 0.0\n",
    "    total = 0.0\n",
    "    correct = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "        # sets gradients of all model parameters to zero\n",
    "        net.zero_grad()\n",
    "        # propagate inputs forward to get a prediction\n",
    "        pred = net.forward(images.cuda().view(-1, 784),False)\n",
    "        # compute loss and stats of performance\n",
    "        loss = net.elbo(images, labels)\n",
    "        total_loss += loss\n",
    "        total += labels.size(0)\n",
    "        correct += (pred.argmax(-1) == labels.cuda()).sum().item()\n",
    "        # propagate backward\n",
    "        loss.backward()\n",
    "        # optimise parameters of the network\n",
    "        optimizer.step()\n",
    "    # print performance for this epoch\n",
    "    print(\n",
    "        f\"Epoch {epoch}: loss {total_loss:.5f} accuracy {correct / total * 100:.5f}\")    \n",
    "    \n",
    "def test_bnn(net, test_loader):\n",
    "    # computes test accuracy for our ensemble by sampling from the distribution of parameters\n",
    "    net.eval()\n",
    "    correct = 0\n",
    "    corrects = np.zeros(SAMPLES+1, dtype=int)\n",
    "    \n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "        outputs = torch.zeros(SAMPLES+1, BATCH_SIZE, CLASSES)\n",
    "        # sampling!\n",
    "        for i in range(SAMPLES):\n",
    "            outputs[i] = net(images, sample=True)\n",
    "        outputs[SAMPLES] = net(images, sample=False)\n",
    "        output = outputs.mean(0)\n",
    "        preds = preds = outputs.max(2, keepdim=True)[1]\n",
    "        pred = output.max(1, keepdim=True)[1] # index of max log-probability\n",
    "        corrects += preds.eq(labels.view_as(pred)).sum(dim=1).squeeze().cpu().numpy()\n",
    "        correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "    for index, num in enumerate(corrects):\n",
    "        if index < SAMPLES:\n",
    "            print('Component {} of Ensemble Accuracy: {}/{}'.format(index, num, TEST_SIZE))\n",
    "        else:\n",
    "            print('Posterior Mean Accuracy: {}/{}'.format(num, TEST_SIZE))\n",
    "    print('Ensemble Accuracy: {}/{}'.format(correct, TEST_SIZE))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Be patient, BNNs take longer to train!\n",
      "Epoch 0: loss 4674491.50000 accuracy 88.63200\n",
      "Epoch 1: loss 4225883.00000 accuracy 95.46200\n",
      "Epoch 2: loss 3766022.25000 accuracy 96.57000\n",
      "Epoch 3: loss 3296221.50000 accuracy 96.89000\n",
      "Epoch 4: loss 2838669.00000 accuracy 96.74400\n",
      "Epoch 5: loss 2417406.50000 accuracy 96.02200\n",
      "Epoch 6: loss 2044851.37500 accuracy 95.88200\n",
      "Epoch 7: loss 1719999.37500 accuracy 95.42200\n",
      "Epoch 8: loss 1441399.00000 accuracy 95.12800\n",
      "Epoch 9: loss 1207458.62500 accuracy 95.03600\n",
      "Component 0 of Ensemble Accuracy: 9409/10000\n",
      "Component 1 of Ensemble Accuracy: 9387/10000\n",
      "Component 2 of Ensemble Accuracy: 9411/10000\n",
      "Component 3 of Ensemble Accuracy: 9430/10000\n",
      "Component 4 of Ensemble Accuracy: 9429/10000\n",
      "Component 5 of Ensemble Accuracy: 9394/10000\n",
      "Component 6 of Ensemble Accuracy: 9410/10000\n",
      "Component 7 of Ensemble Accuracy: 9383/10000\n",
      "Component 8 of Ensemble Accuracy: 9445/10000\n",
      "Component 9 of Ensemble Accuracy: 9411/10000\n",
      "Posterior Mean Accuracy: 9695/10000\n",
      "Ensemble Accuracy: 9693/10000\n"
     ]
    }
   ],
   "source": [
    "print(\"Be patient, BNNs take longer to train!\")\n",
    "\n",
    "bnn = BNN().to(DEVICE)\n",
    "optimizer = torch.optim.Adam(bnn.parameters(), lr=0.001)\n",
    "for epoch in range(TRAIN_EPOCHS):\n",
    "    train_bnn(bnn, optimizer, epoch, train_loader)\n",
    "test_bnn(bnn, test_loader)\n",
    "# that's all folks!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "Now we can see that the test accuracy is similar for all three networks (the network with Sklearn achieved 97%, the non bayesian PyTorch version achieved 97.64% and our Bayesian implementation obtained 96.93%). This, however, is quite different if we train our BNN for longer, as these usually require more epochs. However, independently of the accuracy, our BNN will be much more useful. As we said before, we can prune the network easily and we have a notion of uncertainty in our predictions (which we generate by sampling many times). The fact that the network in Sklearn with a more shallower architecture performs as well as the deeper version might indicate that more layers are not necessary. But to be sure of this we would need to explore much better the performance of these networks by testing different hyperparameter configuration. \n",
    "\n",
    "Now the question is: **How do we include the reject option in our network?** (this is, allow it to say \"I don't know\"): Well, again, we will sample many times (perhaps more than 10, let's say 100), which will give us a much better estimation of the probability of a digit belonging to a class. Then we will need to set a threshold (let's say 0.2) and we will reject classifying every digit for which at least we are not a 20% sure that it belongs to a specific class! This is, if the network is not confident up to a threshold in its prediction, it will reject classifying that example. If you are interested in taking a better look at how to do this I recommend this tutorial: \n",
    "https://towardsdatascience.com/making-your-neural-network-say-i-dont-know-bayesian-nns-using-pyro-and-pytorch-b1c24e6ab8cd\n",
    "\n",
    "In this tutorial we have implemented three multilayer perceptrons with the well-known ReLU activation function, one with Sklearn and two with PyTorch and used one of the most well-known datasets in computer vision (MNIST, for handwritten digit recognition). We have also learnt about the usefulness of probabilistic neural networks and got some intuition about how to implement these!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
