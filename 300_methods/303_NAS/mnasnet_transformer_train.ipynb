{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HjNCQftEBkXc"
   },
   "source": [
    "# MobileNetV3\n",
    "\n",
    "You can read this [website](https://sh-tsang.medium.com/reading-mnasnet-platform-aware-neural-architecture-search-for-mobile-image-classification-b042aaef66f7) to study MnasNet.\n",
    "\n",
    "<div style=\"text-align: center\">\n",
    "    <img src=\"./assets/mnasnet_architecture.png\" width=\"100%\">\n",
    "</div>\n",
    "\n",
    "[paper]: https://arxiv.org/abs/1807.11626\n",
    "\n",
    "### Data Processing\n",
    "\n",
    "As always, we'll start by importing all the necessary modules. We have a few new imports here:\n",
    "- `lr_scheduler` for using the one cycle learning rate scheduler\n",
    "- `namedtuple` for handling ResNet configurations\n",
    "- `os` and `shutil` for handling custom datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NilSkBKhPthJ"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "import torch.utils.data as data\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "\n",
    "from sklearn import decomposition\n",
    "from sklearn import manifold\n",
    "from tqdm.notebook import trange, tqdm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import copy\n",
    "from collections import namedtuple\n",
    "import pandas as pd\n",
    "import os, random, shutil, time, math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s7NNtOWlBkXh"
   },
   "source": [
    "Next, we'll set the random seeds for reproducability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4QmwmcXuPuLo"
   },
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mXgw2VdpBkXk"
   },
   "source": [
    "We'll be using our own dataset instead of using one provided by `torchvision.datasets`.\n",
    "\n",
    "The url of the CUB200-2011 dataset can be found on its [website](http://www.vision.caltech.edu/visipedia/CUB-200-2011.html). However, the dataset is now hosted on Google Drive which has problems with files exceeding their download quota. \n",
    "\n",
    "Instead, we'll download the dataset from [Kaggle](https://www.kaggle.com) using the [kaggle-api](https://github.com/Kaggle/kaggle-api) which needs to be installed with `pip install kaggle`.\n",
    "\n",
    "First, we need to have a Kaggle account in order to generate an API key. Once we're logged on to Kaggle, we need to go to `https://www.kaggle.com/<username>/account` and click `Generate New API Token` which will download a `kaggle.json` file. Place this at `~/.kaggle/kaggle.json` (or, if you're on Windows, at `C:\\Users\\<Windows-username>\\.kaggle\\kaggle.json`). \n",
    "\n",
    "If we are using Google Colab, an alternative is simply set the environment variables `KAGGLE_USERNAME` and `KAGGLE_KEY` to their respective values. This is what we do below.\n",
    "\n",
    "Next, we'll download the dataset from where it is hosted on Kaggle, at: https://www.kaggle.com/veeralakrishna/200-bird-species-with-11788-images. This is done with `!kaggle datasets download <kaggle-url> --unzip`, where the `<kaggle-url>` is the dataset URL after the `kaggle.com` part, which is `veeralakrishna/200-bird-species-with-11788-images` for this dataset.\n",
    "\n",
    "[`datasets.utils`](https://github.com/pytorch/vision/blob/master/torchvision/datasets/utils.py) contains some functionality for downloading and extract data which means we don't have to write it ourselves.\n",
    "\n",
    "We use the `extract_archive` function, which extracts a file to a given root folder. We should now have a `./../data/CUB_200_2011` folder which contains our entire dataset.\n",
    "\n",
    "Put `CUB_200_2011.tgz` to `./../data` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "WhEL6z-dBkXl",
    "outputId": "77831537-5477-42f9-d12d-76ff0ceb14c9"
   },
   "outputs": [],
   "source": [
    "# REPLACE THESE WITH YOUR OWN KAGGLE USERNAME AND KEY\n",
    "#os.environ['KAGGLE_USERNAME'] = 'YOUR_KAGGLE_USERNAME_HERE'\n",
    "#os.environ['KAGGLE_KEY'] = 'YOUR_KAGGLE_KEY_HERE'\n",
    "\n",
    "#!pip install kaggle\n",
    "#!kaggle datasets download veeralakrishna/200-bird-species-with-11788-images --unzip\n",
    "\n",
    "ROOT = './../data'\n",
    "\n",
    "datasets.utils.extract_archive('./../data/CUB_200_2011.tgz', ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "meOmXEbEBkXo"
   },
   "source": [
    "To handle using custom datasets, torchvision provides a [`datasets.ImageFolder`](https://pytorch.org/vision/stable/datasets.html#torchvision.datasets.ImageFolder) class. \n",
    "\n",
    "`ImageFolder` expects data to be stored in the following way:\n",
    "\n",
    "```\n",
    "root/class_x/xxy.png\n",
    "root/class_x/xxz.jpg\n",
    "\n",
    "root/class_y/123.jpeg\n",
    "root/class_y/nsdf3.png\n",
    "root/class_y/asd932_.jpg\n",
    "```\n",
    "\n",
    "That is, each folder in the root directory is the name of a class, and within each of those folders are the images that correspond to that class. The images in the downloaded dataset are currently in the form of:\n",
    "\n",
    "```\n",
    "CUB_200_2011/images/class_a/image_1.jpg\n",
    "CUB_200_2011/images/class_a/image_2.jpg\n",
    "\n",
    "CUB_200_2011/images/class_b/image_1.jpg\n",
    "CUB_200_2011/images/class_b/image_2.jpg\n",
    "\n",
    "CUB_200_2011/images/class_c/image_1.jpg\n",
    "CUB_200_2011/images/class_c/image_2.jpg\n",
    "```\n",
    "\n",
    "This means we could call `datasets.ImageFolder(root = ./../data/CUB_200_2011/images)` and it would load all of the data. However, we want to split our data into train and test splits. This could be done with `data.random_split`, which we have used in the past to create our validation sets - but we will show how to manually create a `train` and `test` folder and store the relevant images in those folders. This way means that we only need to create a train/test split once and re-use it each time we re-run the notebook\n",
    "\n",
    "We first set a `TRAIN_RATIO` which will decide what percentage of the images per class are used to make up the training set, with the remainder making up the test set. We create a `train` and `test` folder within the `CUB_200_2011` folder - after first deleting them if they already exist. Then, we get a list of all classes and loop through each class. For each class we get the image names, use the first `TRAIN_RATIO` of them for the training set and the remainder for the test set. We then copy - with `shutil.copyfile` - each of the images into their respective `train` or `test` folder. It is usually better to copy, rather than move, the images to create your custom splits just in case we accidentally mess up somewhere.\n",
    "\n",
    "After running the below cell we have our training set as:\n",
    "\n",
    "```\n",
    "CUB_200_2011/images/train/class_a/image_1.jpg\n",
    "CUB_200_2011/images/train/class_a/image_2.jpg\n",
    "\n",
    "CUB_200_2011/images/train/class_b/image_1.jpg\n",
    "CUB_200_2011/images/train/class_b/image_2.jpg\n",
    "\n",
    "CUB_200_2011/images/train/class_b/image_1.jpg\n",
    "CUB_200_2011/images/train/class_b/image_2.jpg\n",
    "```\n",
    "\n",
    "and our test set as:\n",
    "\n",
    "```\n",
    "CUB_200_2011/images/test/class_a/image_48.jpg\n",
    "CUB_200_2011/images/test/class_a/image_49.jpg\n",
    "\n",
    "CUB_200_2011/images/test/class_b/image_48.jpg\n",
    "CUB_200_2011/images/test/class_b/image_49.jpg\n",
    "\n",
    "CUB_200_2011/images/test/class_c/image_48.jpg\n",
    "CUB_200_2011/images/test/class_c/image_49.jpg\n",
    "```\n",
    "\n",
    "<font color=red>This train/test split only needs to be created once and does not need to be created again on subsequent runs.</font>\n",
    "\n",
    "**Note:** `ImageFolder` will only load files that have image related extensions, i.e. jpg/jpeg/png, so if there was, for example, a `.txt` file in one of the class folders then it would not be loaded with the images. If we wanted more flexibility when deciding which files to load or not - such as not loading .png images or loading images with an esoteric format - then we could either use the `is_valid_file` argument of the `ImageFolder` class or use [`DatasetFolder`](https://pytorch.org/vision/stable/datasets.html#torchvision.datasets.DatasetFolder) and provide a list of valid extensions to the `extensions` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tnhJlc18BkXp"
   },
   "outputs": [],
   "source": [
    "TRAIN_RATIO = 0.8\n",
    "\n",
    "data_dir = os.path.join(ROOT, 'CUB_200_2011')\n",
    "images_dir = os.path.join(data_dir, 'images')\n",
    "train_dir = os.path.join(data_dir, 'train')\n",
    "test_dir = os.path.join(data_dir, 'test')\n",
    "\n",
    "if os.path.exists(train_dir):\n",
    "    shutil.rmtree(train_dir) \n",
    "if os.path.exists(test_dir):\n",
    "    shutil.rmtree(test_dir)\n",
    "    \n",
    "os.makedirs(train_dir)\n",
    "os.makedirs(test_dir)\n",
    "\n",
    "classes = os.listdir(images_dir)\n",
    "\n",
    "for c in classes:\n",
    "    class_dir = os.path.join(images_dir, c)\n",
    "    \n",
    "    images = os.listdir(class_dir)\n",
    "       \n",
    "    n_train = int(len(images) * TRAIN_RATIO)\n",
    "    \n",
    "    train_images = images[:n_train]\n",
    "    test_images = images[n_train:]\n",
    "    \n",
    "    os.makedirs(os.path.join(train_dir, c), exist_ok = True)\n",
    "    os.makedirs(os.path.join(test_dir, c), exist_ok = True)\n",
    "    \n",
    "    for image in train_images:\n",
    "        image_src = os.path.join(class_dir, image)\n",
    "        image_dst = os.path.join(train_dir, c, image) \n",
    "        shutil.copyfile(image_src, image_dst)\n",
    "        \n",
    "    for image in test_images:\n",
    "        image_src = os.path.join(class_dir, image)\n",
    "        image_dst = os.path.join(test_dir, c, image) \n",
    "        shutil.copyfile(image_src, image_dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SFb7SBBSBkXr"
   },
   "source": [
    "Now we've got our train/test splits we can go ahead and calculate the mean and standard deviation (std) of our dataset to normalize it. We're actually going to use a pre-trained model in this notebook so will be using the mean and std desired by the pre-trained data, so we don't actually have to calculate this - however it is left as an example.\n",
    "\n",
    "Calculating the mean and std is slightly different than when using a dataset provided by torchvision as those datasets have all of the images stored as numpy arrays in the data's `data` attribute, whilst datasets loaded by `ImageFolder` and `DataFolder` do not.\n",
    "\n",
    "First, we load the `train_data` from the `train` folder. Remember: the mean and std must only be calculated from the training data. This will load PIL images by default so we pass the `ToTensor` transform which converts all the PIL images to tensors and scales them from 0-255 to 0-1.\n",
    "\n",
    "We then loop through each image and calculate the mean and std across the height and width dimensions with `dim = (1,2)`, summing all the means and stds and then finding the average by dividing them by the number of examples, `len(train_data)`.\n",
    "\n",
    "Again, this only needs to be calculated once per dataset and the means and stds calculated here can be re-used without calculating them for other runs. The exception to this is if we used a different train/test split, then we would need to calculate these again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "AORf1yn3Pw4H",
    "outputId": "2bc3e5b6-210d-430e-a069-2a5cf6235220"
   },
   "outputs": [],
   "source": [
    "train_data = datasets.ImageFolder(root = train_dir, \n",
    "                                  transform = transforms.ToTensor())\n",
    "\n",
    "means = torch.zeros(3)\n",
    "stds = torch.zeros(3)\n",
    "\n",
    "for img, label in train_data:\n",
    "    means += torch.mean(img, dim = (1,2))\n",
    "    stds += torch.std(img, dim = (1,2))\n",
    "\n",
    "means /= len(train_data)\n",
    "stds /= len(train_data)\n",
    "    \n",
    "print(f'Calculated means: {means}')\n",
    "print(f'Calculated stds: {stds}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-1n1wNalBkXv"
   },
   "source": [
    "Now to actually load our data. As we are going to be using a pre-trained model we will need to ensure that our images are the same size and have the same normalization as those used to train the model - which we find on the torchvision [models](https://pytorch.org/vision/stable/models.html) page.\n",
    "\n",
    "We use the same data augmentation as always: randomly rotating, flipping horizontally and cropping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gb-ZmsUOBkXv"
   },
   "outputs": [],
   "source": [
    "pretrained_size = 224\n",
    "pretrained_means = [0.485, 0.456, 0.406]\n",
    "pretrained_stds= [0.229, 0.224, 0.225]\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "                           transforms.Resize(pretrained_size),\n",
    "                           transforms.RandomRotation(5),\n",
    "                           transforms.RandomHorizontalFlip(0.5),\n",
    "                           transforms.RandomCrop(pretrained_size, padding = 10),\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize(mean = pretrained_means, \n",
    "                                                std = pretrained_stds)\n",
    "                       ])\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "                           transforms.Resize(pretrained_size),\n",
    "                           transforms.CenterCrop(pretrained_size),\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize(mean = pretrained_means, \n",
    "                                                std = pretrained_stds)\n",
    "                       ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kYMHIIXMBkXy"
   },
   "source": [
    "We load our data with our transforms..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l-89cb0XBkXz"
   },
   "outputs": [],
   "source": [
    "train_data = datasets.ImageFolder(root = train_dir, \n",
    "                                  transform = train_transforms)\n",
    "\n",
    "test_data = datasets.ImageFolder(root = test_dir, \n",
    "                                 transform = test_transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HdWme8gvBkX1"
   },
   "source": [
    "...create the validation split..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9OQN9jG2P6p8"
   },
   "outputs": [],
   "source": [
    "VALID_RATIO = 0.9\n",
    "\n",
    "n_train_examples = int(len(train_data) * VALID_RATIO)\n",
    "n_valid_examples = len(train_data) - n_train_examples\n",
    "\n",
    "train_data, valid_data = data.random_split(train_data, \n",
    "                                           [n_train_examples, n_valid_examples])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZHKEdansBkX4"
   },
   "source": [
    "...and then overwrite the validation transforms, making sure to do a `deepcopy` to stop this also changing the training data transforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9A-RD7IdPy7K"
   },
   "outputs": [],
   "source": [
    "valid_data = copy.deepcopy(valid_data)\n",
    "valid_data.dataset.transform = test_transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8j-X6J-zBkX7"
   },
   "source": [
    "To make sure nothing has messed up we'll print the number of examples in each of the data splits - ensuring they add up to the number of examples indicated on the [CUB200-2011 dataset website](http://www.vision.caltech.edu/visipedia/CUB-200-2011.html) (11,788)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "-4XfHZL1xVLT",
    "outputId": "2a862f4f-25fe-4d58-8604-27ac1722d120"
   },
   "outputs": [],
   "source": [
    "print(f'Number of training examples: {len(train_data)}')\n",
    "print(f'Number of validation examples: {len(valid_data)}')\n",
    "print(f'Number of testing examples: {len(test_data)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zgCBaMUjBkX9"
   },
   "source": [
    "Next, we'll create the iterators with the largest batch size that fits on our GPU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LlF1vQHHRESs"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100\n",
    "\n",
    "train_iterator = data.DataLoader(train_data, \n",
    "                                 shuffle = True, \n",
    "                                 batch_size = BATCH_SIZE)\n",
    "\n",
    "valid_iterator = data.DataLoader(valid_data, \n",
    "                                 batch_size = BATCH_SIZE)\n",
    "\n",
    "test_iterator = data.DataLoader(test_data, \n",
    "                                batch_size = BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cwUrAarMBkYA"
   },
   "source": [
    "To ensure the images have been processed correctly we can plot a few of them - ensuring we re-normalize the images so their colors look right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MjvuGeRMBkYB"
   },
   "outputs": [],
   "source": [
    "def normalize_image(image):\n",
    "    image_min = image.min()\n",
    "    image_max = image.max()\n",
    "    image.clamp_(min = image_min, max = image_max)\n",
    "    image.add_(-image_min).div_(image_max - image_min + 1e-5)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XZVxkhKxBkYD"
   },
   "outputs": [],
   "source": [
    "def plot_images(images, labels, classes, normalize = True):\n",
    "    n_images = len(images)\n",
    "\n",
    "    rows = int(np.sqrt(n_images))\n",
    "    cols = int(np.sqrt(n_images))\n",
    "    fig = plt.figure(figsize = (15, 15))\n",
    "    for i in range(rows*cols):\n",
    "        ax = fig.add_subplot(rows, cols, i+1)\n",
    "        \n",
    "        image = images[i]\n",
    "\n",
    "        if normalize:\n",
    "            image = normalize_image(image)\n",
    "\n",
    "        ax.imshow(image.permute(1, 2, 0).cpu().numpy())\n",
    "        label = classes[labels[i]]\n",
    "        ax.set_title(label)\n",
    "        ax.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SaY46boZBkYG"
   },
   "source": [
    "We can see the images look fine, however the names of the classes provided by the folders containing the images are a little long and sometimes overlap with neighbouring images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 862
    },
    "colab_type": "code",
    "id": "TueBO2OxRQiq",
    "outputId": "0bbc6ba3-0e1b-47b3-ed1e-1b5dcc373b90"
   },
   "outputs": [],
   "source": [
    "N_IMAGES = 25\n",
    "\n",
    "images, labels = zip(*[(image, label) for image, label in \n",
    "                           [train_data[i] for i in range(N_IMAGES)]])\n",
    "\n",
    "classes = test_data.classes\n",
    "\n",
    "plot_images(images, labels, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FMd7wL88BkYJ"
   },
   "source": [
    "One way to solve the issue with the names of the classes would have been to manually change the names of the folders before we copied them over into the `train` and `test` folders. \n",
    "\n",
    "Another approach is to directly change the names of each class provided by the dataset's `.classes`. We'll make a `format_label` function which will strip off the number at the start of each class and convert them into title case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zI3n_KkxBkYK"
   },
   "outputs": [],
   "source": [
    "def format_label(label):\n",
    "    label = label.split('.')[-1]\n",
    "    label = label.replace('_', ' ')\n",
    "    label = label.title()\n",
    "    label = label.replace(' ', '')\n",
    "    return label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lGfw4je0BkYO"
   },
   "source": [
    "Let's change the class names and re-plot the images with their new class names.\n",
    "\n",
    "No more overlapping!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 862
    },
    "colab_type": "code",
    "id": "LPU5qt4hBkYO",
    "outputId": "1ac79924-9ea9-4398-da8a-f833f456f8de"
   },
   "outputs": [],
   "source": [
    "test_data.classes = [format_label(c) for c in test_data.classes]\n",
    "classes = test_data.classes\n",
    "plot_images(images, labels, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MiEhadDsBkYR"
   },
   "source": [
    "### Defining the Model\n",
    "\n",
    "As we have a relatively small dataset - 12,000 images - with a very small amount of examples per class - 60 images - we'll be using a pre-trained model.\n",
    "\n",
    "Torchvision provides pre-trained models for all of the standard MobileNetV3 variants.\n",
    "\n",
    "First, we load the pre-trained Mnasnet model from [website](https://pytorch.org/vision/stable/models/mnasnet.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 83,
     "referenced_widgets": [
      "e0d5f6ba52604352a6484d1907385629",
      "73733f1d2af0455890e9d6f695321d89",
      "9e26aa8e760c41c4bcd81d121abb8a54",
      "d1868678a8d2487a85394a7eb6d85608",
      "dcd14c4f39ba40b1924df3921551a759",
      "2bfc54026555415b819aef84868882a0",
      "33af98c97e6141f3bfc08f9cedb4dbd0",
      "6237aa6a8e1843789e503390f5dcbd0d"
     ]
    },
    "colab_type": "code",
    "id": "qsjfsiUhRZUD",
    "outputId": "cdb95e54-4c8c-4fcf-a893-ed88a9c5650f"
   },
   "outputs": [],
   "source": [
    "model = models.mnasnet1_3(weights=models.MNASNet1_3_Weights.IMAGENET1K_V1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WP0QAUDUBkYs"
   },
   "source": [
    "We can see that the final linear layer for the classification, `fc`, has a 1000-dimensional output as it was pre-trained on the ImageNet dataset, which has 1000 classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "fPwZPcoZBkYt",
    "outputId": "76004c0e-53fe-442d-d504-2947b61efe62"
   },
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9te929fMBkYw"
   },
   "source": [
    "Our dataset, however, only has 200 classes, so we first create a new linear layer with the required dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jf6xf2WVcNfY"
   },
   "outputs": [],
   "source": [
    "IN_FEATURES = model.classifier[1].in_features \n",
    "OUTPUT_DIM = len(test_data.classes)\n",
    "print('OUTPUT_DIM is {}'.format(OUTPUT_DIM))\n",
    "\n",
    "fc = nn.Linear(IN_FEATURES, OUTPUT_DIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZMFyzFh-BkYy"
   },
   "source": [
    "Then, we replace the pre-trained model's linear layer with our own, randomly initialized linear layer.\n",
    "\n",
    "**Note:** even if our dataset had 1000 classes, the same as ImageNet, we would still remove the linear layer and replace it with a randomly initialized one as our classes are not equal to those of ImageNet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9Ol3c207BkYz"
   },
   "outputs": [],
   "source": [
    "model.classifier[1] = fc\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TAlT4YOtBkY7"
   },
   "source": [
    "We can also see the number of parameters in our model - noticing that ResNet50 only has ~24M parameters compared to VGG11's ~129M. This is mostly due to the lack of high dimensional linear layers which have been replaced by more parameter efficient convolutional layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "RwlzAhurBkY7",
    "outputId": "3ea4d86b-08d2-4b83-e879-b15c61649e8b"
   },
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pAmuYsW0BkY9"
   },
   "source": [
    "### Training the Model\n",
    "\n",
    "Next we'll move on to training our model. As in previous notebooks, we'll use the learning rate finder to set a suitable learning rate for our model.\n",
    "\n",
    "We start by initializing an optimizer with a very low learning rate, defining a loss function (`criterion`) and device, and then placing the model and the loss function on to the device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4Cuqluk1ZNOW"
   },
   "outputs": [],
   "source": [
    "START_LR = 1e-7\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=START_LR)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6Kul0S5jBkZA"
   },
   "source": [
    "We define the learning rate finder class.\n",
    "\n",
    "See notebook 3 for a reminder on how this works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2JSiXTj3ZfwJ"
   },
   "outputs": [],
   "source": [
    "class LRFinder:\n",
    "    def __init__(self, model, optimizer, criterion, device):\n",
    "        self.optimizer = optimizer\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.device = device\n",
    "        \n",
    "        torch.save(model.state_dict(), 'init_params.pt')\n",
    "\n",
    "    def range_test(self, iterator, end_lr = 10, num_iter = 100, \n",
    "                   smooth_f = 0.05, diverge_th = 5):\n",
    "        lrs = []\n",
    "        losses = []\n",
    "        best_loss = float('inf')\n",
    "\n",
    "        lr_scheduler = ExponentialLR(self.optimizer, end_lr, num_iter)\n",
    "        \n",
    "        iterator = IteratorWrapper(iterator)\n",
    "        \n",
    "        for iteration in range(num_iter):\n",
    "            loss = self._train_batch(iterator)\n",
    "\n",
    "            #update lr\n",
    "            lr_scheduler.step()\n",
    "            \n",
    "            lrs.append(lr_scheduler.get_lr()[0])\n",
    "\n",
    "            if iteration > 0:\n",
    "                loss = smooth_f * loss + (1 - smooth_f) * losses[-1]\n",
    "                \n",
    "            if loss < best_loss:\n",
    "                best_loss = loss\n",
    "\n",
    "            losses.append(loss)\n",
    "            \n",
    "            if loss > diverge_th * best_loss:\n",
    "                print(\"Stopping early, the loss has diverged\")\n",
    "                break     \n",
    "        #reset model to initial parameters\n",
    "        model.load_state_dict(torch.load('init_params.pt'))\n",
    "        return lrs, losses\n",
    "\n",
    "    def _train_batch(self, iterator):\n",
    "        self.model.train()\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        x, y = iterator.get_batch()\n",
    "        \n",
    "        x = x.to(self.device)\n",
    "        y = y.to(self.device)\n",
    "        \n",
    "        y_pred = self.model(x)\n",
    "        #y_pred, _ = self.model(x)\n",
    "                \n",
    "        loss = self.criterion(y_pred, y)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        self.optimizer.step()\n",
    "        return loss.item()\n",
    "\n",
    "class ExponentialLR(_LRScheduler):\n",
    "    def __init__(self, optimizer, end_lr, num_iter, last_epoch=-1):\n",
    "        self.end_lr = end_lr\n",
    "        self.num_iter = num_iter\n",
    "        super(ExponentialLR, self).__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        curr_iter = self.last_epoch + 1\n",
    "        r = curr_iter / self.num_iter\n",
    "        return [base_lr * (self.end_lr / base_lr) ** r for base_lr in self.base_lrs]\n",
    "\n",
    "class IteratorWrapper:\n",
    "    def __init__(self, iterator):\n",
    "        self.iterator = iterator\n",
    "        self._iterator = iter(iterator)\n",
    "\n",
    "    def __next__(self):\n",
    "        try:\n",
    "            inputs, labels = next(self._iterator)\n",
    "        except StopIteration:\n",
    "            self._iterator = iter(self.iterator)\n",
    "            inputs, labels, *_ = next(self._iterator)\n",
    "        return inputs, labels\n",
    "\n",
    "    def get_batch(self):\n",
    "        return next(self)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "meQZv6RkBkZF"
   },
   "source": [
    "We then define our learning rate finder and run the range test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "fxtlrpfWZk8t",
    "outputId": "d4a73dc9-ab5a-428b-a0ed-c3d76fd4dc3e"
   },
   "outputs": [],
   "source": [
    "END_LR = 10\n",
    "NUM_ITER = 100\n",
    "\n",
    "lr_finder = LRFinder(model, optimizer, criterion, device)\n",
    "lrs, losses = lr_finder.range_test(train_iterator, END_LR, NUM_ITER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w-QTkQb6BkZJ"
   },
   "source": [
    "Next, we define a function to plot the results of the range test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BBzT3twzZnAW"
   },
   "outputs": [],
   "source": [
    "def plot_lr_finder(lrs, losses, skip_start = 5, skip_end = 5):\n",
    "    if skip_end == 0:\n",
    "        lrs = lrs[skip_start:]\n",
    "        losses = losses[skip_start:]\n",
    "    else:\n",
    "        lrs = lrs[skip_start:-skip_end]\n",
    "        losses = losses[skip_start:-skip_end]\n",
    "    \n",
    "    fig = plt.figure(figsize = (16,8))\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    ax.plot(lrs, losses)\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_xlabel('Learning rate')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.grid(True, 'both', 'x')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XG-dUhQdBkZN"
   },
   "source": [
    "We can see that the loss reaches a minimum at around $3x10^{-3}$.\n",
    "\n",
    "A good learning rate to choose here would be the middle of the steepest downward curve - which is around $1x10^{-3}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 501
    },
    "colab_type": "code",
    "id": "BnIQz0g4Zp5n",
    "outputId": "ff827ee8-c933-49a9-f5ac-12d6edf50c50"
   },
   "outputs": [],
   "source": [
    "plot_lr_finder(lrs, losses, skip_start = 30, skip_end = 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-Uq_OykQBkZU"
   },
   "source": [
    "Next up, we set the learning rate scheduler. A learning rate scheduler dynamically alters the learning rate whilst the model is training. We'll be using the one cycle learning rate scheduler, however [many](https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate) schedulers are available in PyTorch.\n",
    "\n",
    "The one cycle learning rate scheduler starts with a small initial learning rate which is gradually increased to a maximum value - the value found by our learning rate finder - it then slowly decreases the learning rate to a final value smaller than the initial learning rate. This learning rate is updated after every parameter update step, i.e. after every training batch. For our model, the learning rate for the final `fc` layer throughout training will look like:\n",
    "\n",
    "![](./../assets/lr-scheduler.png)\n",
    "\n",
    "As we can see, it starts at slightly less than $1x10^{-4}$ before gradually increasing to the maximum value of $1x10^{-3}$ at around a third of the way through training, then it begins decreasing to almost zero.\n",
    "\n",
    "The different parameter groups defined by the optimizer for the discriminative fine-tuning will all have their own learning rate curves, each with different starting and maximum values.\n",
    "\n",
    "The hypothesis is that the initial stage where the learning rate increases is a \"warm-up\" phase is used to get the model into a generally good area of the loss landscape. The middle of the curve, where the learning rate is at maximum is supposedly good for acting as a regularization method and prevents the model from overfitting or becoming stuck in saddle points. Finally, the \"cool-down\" phase, where the learning rate decreases, is used to reach small crevices in the loss surface which have a lower loss value.\n",
    "\n",
    "The one cycle learning rate also cycles the momentum of the optimizer. The momentum is cycled from a maximum value, down to a minimum and then back up to the maximum where it is held constant for the last few steps. The default maximum and minimum values of momentum used by PyTorch's one cycle learning rate scheduler should be sufficient and we will not change them.\n",
    "\n",
    "To set-up the one cycle learning rate scheduler we need the total number of steps that will occur during training. We simply get this by multiplying the number of epochs with the number of batches in the training iterator, i.e. number of parameter updates. We get the maximum learning rate for each parameter group and pass this to `max_lr`. **Note:** if you only pass a single learning rate and not a list of learning rates then the scheduler will assume this learning rate should be used for all parameters and will **not** do discriminative fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "riS8zV2IxVMK"
   },
   "outputs": [],
   "source": [
    "FOUND_LR = 1e-3\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=FOUND_LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qsvplN-tBkZX"
   },
   "source": [
    "One other thing we are going to implement is top-k accuracy. Our task is to classify an image into one of 200 classes of bird, however some of these classes look very similar and it is even difficult for a human to correctly label them. So, maybe we should be more lenient when calculating accuracy? \n",
    "\n",
    "One method of solving this is using top-k accuracy, where the prediction is labelled correct if the correct label is in the top-k predictions, instead of just being the first. Our `calculate_topk_accuracy` function calculates the top-1 accuracy as well as the top-k accuracy, with $k=5$ by default.\n",
    "\n",
    "We use `.reshape` instead of view here as the slices into tensors cause them to become non-contiguous which means `.view` throws an error. As a rule of thumb, if you are aiming to change the size/shape of sliced tensors then you should probably use `.reshape` instead of `.view`.\n",
    "\n",
    "**Note:** our value of k should be chosen sensibly. If we had a dataset with 10 classes then a k of 5 isn't really that informative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t_EKb_cNadbI"
   },
   "outputs": [],
   "source": [
    "def calculate_accuracy(y_pred, y):\n",
    "    top_pred = y_pred.argmax(1, keepdim=True)\n",
    "    correct = top_pred.eq(y.view_as(top_pred)).sum()\n",
    "    acc = correct.float() / y.shape[0]\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LhVOm-xcBkZa"
   },
   "source": [
    "Next up is the training function. This is similar to all the previous notebooks, but with the addition of the `scheduler` and calculating/returning top-k accuracy.\n",
    "\n",
    "The scheduler is updated by calling `scheduler.step()`. This should always be called **after** `optimizer.step()` or else the first learning rate of the scheduler will be skipped. \n",
    "\n",
    "Not all schedulers need to be called after each training batch, some are only called after each epoch. In that case, the scheduler does not need to be passed to the `train` function and can be called in the main training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qr6Je0jMafsp"
   },
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, device):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    top3_train_accuracy = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for (x, y) in tqdm(iterator, desc=\"Training\", leave=False):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        y_pred = model(x)\n",
    "        #y_pred, _ = model(x)\n",
    "\n",
    "        loss = criterion(y_pred, y)\n",
    "\n",
    "        acc = calculate_accuracy(y_pred, y)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "        ps = torch.exp(y_pred)\n",
    "        np_top3_class = ps.topk(3, dim=1)[1].cpu().numpy()\n",
    "        target_numpy = y.cpu().numpy()\n",
    "        top3_train_accuracy += np.mean([1 if target_numpy[i] in np_top3_class[i] else 0 for i in range(0, len(target_numpy))])\n",
    "\n",
    "    return (epoch_loss/len(iterator)), (epoch_acc/len(iterator)), (top3_train_accuracy/len(iterator))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZNCuH35gBkZd"
   },
   "source": [
    "The evaluation function is also similar to previous notebooks, with the addition of the top-k accuracy.\n",
    "\n",
    "As the one cycle scheduler should only be called after each parameter update, it is not called here as we do not update parameters whilst evaluating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "85FW74KaakI_"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion, device):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    top3_test_accuracy = 0\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for (x, y) in tqdm(iterator, desc=\"Evaluating\", leave=False):\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            y_pred = model(x)\n",
    "            #y_pred, _ = model(x)\n",
    "\n",
    "            loss = criterion(y_pred, y)\n",
    "\n",
    "            acc = calculate_accuracy(y_pred, y)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "            \n",
    "            ps = torch.exp(y_pred)\n",
    "            np_top3_class = ps.topk(3, dim=1)[1].cpu().numpy()\n",
    "            target_numpy = y.cpu().numpy()\n",
    "            top3_test_accuracy += np.mean([1 if target_numpy[i] in np_top3_class[i] else 0 for i in range(0, len(target_numpy))])\n",
    "            \n",
    "    return (epoch_loss/len(iterator)), (epoch_acc/len(iterator)), (top3_test_accuracy/len(iterator))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k1KTGJyxBkZf"
   },
   "source": [
    "Next, a small helper function which tells us how long an epoch has taken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wkR6LzakamOV"
   },
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return (elapsed_mins, elapsed_secs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gTUb3MftBkZh"
   },
   "source": [
    "Finally, we can train our model!\n",
    "\n",
    "We get around 80% top-1 and 95% top-5 validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 527
    },
    "colab_type": "code",
    "id": "lvoyX823apEN",
    "outputId": "0cb310cb-9016-4cca-dcbb-df2f04345e99",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "train_stats = pd.DataFrame(columns = ['Epoch', 'Time per epoch', 'Avg time per step', \n",
    "                                      'Train loss', 'Train accuracy', 'Train top-3 accuracy', \n",
    "                                      'Test loss', 'Test accuracy', 'Test top-3 accuracy'])\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in trange(EPOCHS, desc=\"Epochs\"):\n",
    "    start_time = time.monotonic()\n",
    "\n",
    "    train_loss, train_acc, top3_train_acc = train(model, train_iterator, optimizer, criterion, device)\n",
    "    valid_loss, valid_acc, top3_valid_acc = evaluate(model, valid_iterator, criterion, device)\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'mobilenetV3_transfer_train-model.pt')\n",
    "\n",
    "    end_time = time.monotonic()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    time_elapsed = end_time - start_time\n",
    "\n",
    "    df = pd.DataFrame.from_dict(\n",
    "                [ {'Epoch': epoch, 'Time per epoch':time_elapsed, \n",
    "                  'Avg time per step': time_elapsed/len(train_iterator), \n",
    "                  'Train loss' : train_loss, \n",
    "                  'Train accuracy': train_acc, \n",
    "                  'Train top-3 accuracy':top3_train_acc,\n",
    "                  'Test loss' : valid_loss, \n",
    "                  'Test accuracy': valid_acc, \n",
    "                  'Test top-3 accuracy':top3_valid_acc} ] )\n",
    "    train_stats = pd.concat([ train_stats, df], ignore_index=True)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}% | Top3 train Acc: {train_acc*100:.2f}%')  \n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}% | Top3 valid Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Free cuda memoryï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save training log to a csv file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stats.to_csv('train_log_mnasnet_transformer_train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze Training Log\n",
    "\n",
    "Plot test and train accuracy and test and train loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,7))\n",
    "ax = plt.axes()\n",
    "\n",
    "plt.title(\"Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\");\n",
    "\n",
    "x = range(1, len(train_stats['Train loss'].values) + 1)\n",
    "ax.plot(x, train_stats['Train loss'].values, '-g', label='train loss');\n",
    "ax.plot(x, train_stats['Test loss'].values, '-b', label='test loss');\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,7))\n",
    "ax = plt.axes()\n",
    "\n",
    "plt.title(\"Accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\");\n",
    "\n",
    "x = range(1, len(train_stats['Train accuracy'].values) + 1)\n",
    "ax.plot(x, train_stats['Train accuracy'].values, '-g', label='train accuracy');\n",
    "ax.plot(x, train_stats['Test accuracy'].values, '-b', label='test accuracy');\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZFxCXpuxBkZj"
   },
   "source": [
    "The test accuracies are a little lower than the validation accuracies, but not by so much that we should be concerned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "fOla2axMBkZk",
    "outputId": "a8178c45-adaa-4b00-bf59-0a1de0eb4363"
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('mobilenetV3_transfer_train-model.pt'))\n",
    "\n",
    "test_loss, test_acc_1, test_acc_5 = evaluate(model, test_iterator, criterion, device)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc @1: {test_acc_1*100:6.2f}% | ' \\\n",
    "      f'Test Acc @5: {test_acc_5*100:6.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ORmtSernBkZo"
   },
   "source": [
    "### Examining the Model\n",
    "\n",
    "We'll be doing the same examinations of the model as done in previous notebooks.\n",
    "\n",
    "First, we get the predictions for each image in the test set..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AlCBiYxQbi4s"
   },
   "outputs": [],
   "source": [
    "def get_predictions(model, iterator):\n",
    "    model.eval()\n",
    "\n",
    "    images = []\n",
    "    labels = []\n",
    "    probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for (x, y) in iterator:\n",
    "            x = x.to(device)\n",
    "\n",
    "            y_pred = model(x)\n",
    "            #y_pred, _ = model(x)\n",
    "\n",
    "            y_prob = F.softmax(y_pred, dim = -1)\n",
    "            top_pred = y_prob.argmax(1, keepdim = True)\n",
    "\n",
    "            images.append(x.cpu())\n",
    "            labels.append(y.cpu())\n",
    "            probs.append(y_prob.cpu())\n",
    "\n",
    "    images = torch.cat(images, dim = 0)\n",
    "    labels = torch.cat(labels, dim = 0)\n",
    "    probs = torch.cat(probs, dim = 0)\n",
    "\n",
    "    return images, labels, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JmfK-IZ6bmlz"
   },
   "outputs": [],
   "source": [
    "images, labels, probs = get_predictions(model, test_iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "scfRUtDKBkZt"
   },
   "source": [
    "... which we then use to get the predicted labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zR__HoORbpXV"
   },
   "outputs": [],
   "source": [
    "pred_labels = torch.argmax(probs, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NE_xrcm3BkZw"
   },
   "source": [
    "We then use this to plot a confusion matrix.\n",
    "\n",
    "A 200x200 confusion matrix is pretty large, so we've increased the figure size and removed the colorbar. The text is still too small to read, but all we care about is that the diagonals are generally darker than the rest of the matrix - which they are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hupBoXNhbqe_"
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(labels, pred_labels, classes):\n",
    "    fig = plt.figure(figsize = (50, 50));\n",
    "    ax = fig.add_subplot(1, 1, 1);\n",
    "    cm = confusion_matrix(labels, pred_labels);\n",
    "    cm = ConfusionMatrixDisplay(cm, display_labels = classes);\n",
    "    cm.plot(values_format = 'd', cmap = 'Blues', ax = ax)\n",
    "    fig.delaxes(fig.axes[1]) #delete colorbar\n",
    "    plt.xticks(rotation = 90)\n",
    "    plt.xlabel('Predicted Label', fontsize = 50)\n",
    "    plt.ylabel('True Label', fontsize = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "08XMzduObsMa",
    "outputId": "10e2d465-b7dd-4bb3-c3b8-73bd6453e3d5"
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix(labels, pred_labels, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GsUXv-ZHBkZ0"
   },
   "source": [
    "We can then get all of the correct predictions, filter them out, and then sort all of the incorrect predictions based on how confident they were on their incorrect prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lgSkMA9dbzy8"
   },
   "outputs": [],
   "source": [
    "corrects = torch.eq(labels, pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d8DbVGfYb0N_"
   },
   "outputs": [],
   "source": [
    "incorrect_examples = []\n",
    "\n",
    "for image, label, prob, correct in zip(images, labels, probs, corrects):\n",
    "    if not correct:\n",
    "        incorrect_examples.append((image, label, prob))\n",
    "\n",
    "incorrect_examples.sort(reverse = True, key = lambda x: torch.max(x[2], dim = 0).values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N-oeP4ycBkZ4"
   },
   "source": [
    "We can then plot the most incorrectly predicted images along with the predicted class and the actual class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kfbuUGh4b2M4"
   },
   "outputs": [],
   "source": [
    "def plot_most_incorrect(incorrect, classes, n_images, normalize = True):\n",
    "    rows = int(np.sqrt(n_images))\n",
    "    cols = int(np.sqrt(n_images))\n",
    "\n",
    "    fig = plt.figure(figsize = (25, 20))\n",
    "\n",
    "    for i in range(rows*cols):\n",
    "        ax = fig.add_subplot(rows, cols, i+1)\n",
    "        \n",
    "        image, true_label, probs = incorrect[i]\n",
    "        image = image.permute(1, 2, 0)\n",
    "        true_prob = probs[true_label]\n",
    "        incorrect_prob, incorrect_label = torch.max(probs, dim = 0)\n",
    "        true_class = classes[true_label]\n",
    "        incorrect_class = classes[incorrect_label]\n",
    "\n",
    "        if normalize:\n",
    "            image = normalize_image(image)\n",
    "\n",
    "        ax.imshow(image.cpu().numpy())\n",
    "        ax.set_title(f'true label: {true_class} ({true_prob:.3f})\\n' \\\n",
    "                     f'pred label: {incorrect_class} ({incorrect_prob:.3f})')\n",
    "        ax.axis('off')\n",
    "        \n",
    "    fig.subplots_adjust(hspace=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nkj1fB0TBkZ7"
   },
   "source": [
    "From the names of the classes (and a bit of image searching) we can see that the incorrect predictions are usually sensible, e.g. magnolia warbler and cape may warbler, marsh wren and carolina wren.\n",
    "\n",
    "The most incorrectly predicted image might also be a mislabeled example as worm eating warblers do not have a black and white underside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "VL56wXWtb4BT",
    "outputId": "7caa50a4-6f37-4075-b1b1-7fc2a0848fd3"
   },
   "outputs": [],
   "source": [
    "N_IMAGES = 36\n",
    "\n",
    "plot_most_incorrect(incorrect_examples, classes, N_IMAGES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hozwk47TBkZ9"
   },
   "source": [
    "We can also get the representations from the model in order to perform some dimensionality reduction techniques.\n",
    "\n",
    "In this notebook we'll only get the output representations, and not the intermediate ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r5UzPHw29QVi"
   },
   "outputs": [],
   "source": [
    "def get_representations(model, iterator):\n",
    "    model.eval()\n",
    "\n",
    "    outputs = []\n",
    "    intermediates = []\n",
    "    labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        #for (x, y) in iterator:\n",
    "        for (x, y) in tqdm(iterator):\n",
    "            x = x.to(device)\n",
    "\n",
    "            y_pred = model(x)\n",
    "            #y_pred, _ = model(x)\n",
    "\n",
    "            outputs.append(y_pred.cpu())\n",
    "            labels.append(y)\n",
    "        \n",
    "    outputs = torch.cat(outputs, dim = 0)\n",
    "    labels = torch.cat(labels, dim = 0)\n",
    "\n",
    "    return outputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tYvJXnuu9by8"
   },
   "outputs": [],
   "source": [
    "outputs, labels = get_representations(model, train_iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h1R4lhBuBkaA"
   },
   "source": [
    "We can then perform PCA on these representations to plot them in two dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6C0vTaVm9dha"
   },
   "outputs": [],
   "source": [
    "def get_pca(data, n_components = 2):\n",
    "    pca = decomposition.PCA()\n",
    "    pca.n_components = n_components\n",
    "    pca_data = pca.fit_transform(data)\n",
    "    return pca_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ggFP2ccmBkaB"
   },
   "source": [
    "As there are 200 classes, it is difficult for every class to have a unique color. \n",
    "\n",
    "Also a legend with 200 elements is quite large, so we do not plot it - however we leave the code (commented out) to do so if required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pHQfixt09fMO"
   },
   "outputs": [],
   "source": [
    "def plot_representations(data, labels, classes, n_images = None):\n",
    "    if n_images is not None:\n",
    "        data = data[:n_images]\n",
    "        labels = labels[:n_images]\n",
    "                \n",
    "    fig = plt.figure(figsize = (15, 15))\n",
    "    ax = fig.add_subplot(111)\n",
    "    scatter = ax.scatter(data[:, 0], data[:, 1], c = labels, cmap = 'hsv')\n",
    "    #handles, _ = scatter.legend_elements(num = None)\n",
    "    #legend = plt.legend(handles = handles, labels = classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fayYO5S9BkaE"
   },
   "source": [
    "The classes do not seem as well separated as in previous notebooks, although this is most probably due to there being so many classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 863
    },
    "colab_type": "code",
    "id": "TpZQLiLZ9hdh",
    "outputId": "ff8114df-56ea-4d08-c628-d00ced87382c"
   },
   "outputs": [],
   "source": [
    "output_pca_data = get_pca(outputs)\n",
    "plot_representations(output_pca_data, labels, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "49pfWd9XBkaG"
   },
   "source": [
    "Next up, we plot the t-SNE data. As we have a much smaller dataset than the CIFAR datasets used previously we can perform t-SNE on the entire dataset in a reasonable amount of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pz6yxrS29i9D"
   },
   "outputs": [],
   "source": [
    "def get_tsne(data, n_components = 2, n_images = None):\n",
    "    if n_images is not None:\n",
    "        data = data[:n_images]\n",
    "        \n",
    "    tsne = manifold.TSNE(n_components = n_components, random_state = 0)\n",
    "    tsne_data = tsne.fit_transform(data)\n",
    "    return tsne_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P7HWLoL0BkaH"
   },
   "source": [
    "The classes are definitely well separated here - which is usually a good sign."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 863
    },
    "colab_type": "code",
    "id": "fe8v2Z2v9koj",
    "outputId": "41f8e18d-611b-43e9-90b8-8aec133a659c"
   },
   "outputs": [],
   "source": [
    "output_tsne_data = get_tsne(outputs)\n",
    "plot_representations(output_tsne_data, labels, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cGNLTwM-fnQF"
   },
   "source": [
    "We can then plot what a few of the images look like after having gone through the first convolutional layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VDqM5kXc9mU7"
   },
   "outputs": [],
   "source": [
    "def plot_filtered_images(images, filters, n_filters = None, normalize = True):\n",
    "    images = torch.cat([i.unsqueeze(0) for i in images], dim = 0).cpu()\n",
    "    filters = filters.cpu()\n",
    "\n",
    "    if n_filters is not None:\n",
    "        filters = filters[:n_filters]\n",
    "\n",
    "    n_images = images.shape[0]\n",
    "    n_filters = filters.shape[0]\n",
    "\n",
    "    filtered_images = F.conv2d(images, filters)\n",
    "\n",
    "    fig = plt.figure(figsize = (30, 30))\n",
    "\n",
    "    for i in range(n_images):\n",
    "        image = images[i]\n",
    "\n",
    "        if normalize:\n",
    "            image = normalize_image(image)\n",
    "\n",
    "        ax = fig.add_subplot(n_images, n_filters+1, i+1+(i*n_filters))\n",
    "        ax.imshow(image.permute(1,2,0).numpy())\n",
    "        ax.set_title('Original')\n",
    "        ax.axis('off')\n",
    "\n",
    "        for j in range(n_filters):\n",
    "            image = filtered_images[i][j]\n",
    "\n",
    "            if normalize:\n",
    "                image = normalize_image(image)\n",
    "\n",
    "            ax = fig.add_subplot(n_images, n_filters+1, i+1+(i*n_filters)+j+1)\n",
    "            ax.imshow(image.numpy(), cmap = 'bone')\n",
    "            ax.set_title(f'Filter {j+1}')\n",
    "            ax.axis('off');\n",
    "\n",
    "    fig.subplots_adjust(hspace = -0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B3cF1gU3BkaN"
   },
   "source": [
    "We can see that the filters perform many different types of image processing - from edge detection to color inversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 965
    },
    "colab_type": "code",
    "id": "Gknz19kp9onk",
    "outputId": "5ae3049a-2ce5-4f84-d1f3-ddd96931deb2"
   },
   "outputs": [],
   "source": [
    "N_IMAGES = 5\n",
    "N_FILTERS = 7\n",
    "\n",
    "images = [image for image, label in [train_data[i] for i in range(N_IMAGES)]]\n",
    "filters = model.features[0][0].weight.data\n",
    "\n",
    "plot_filtered_images(images, filters, N_FILTERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3PcFkdOFBkaP"
   },
   "source": [
    "Finally, we can plot the values of the filters themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZOGoonSkxVNk"
   },
   "outputs": [],
   "source": [
    "def plot_filters(filters, normalize = True):\n",
    "    filters = filters.cpu()\n",
    "\n",
    "    n_filters = filters.shape[0]\n",
    "\n",
    "    rows = int(np.sqrt(n_filters))\n",
    "    cols = int(np.sqrt(n_filters))\n",
    "\n",
    "    fig = plt.figure(figsize = (30, 15))\n",
    "\n",
    "    for i in range(rows*cols):\n",
    "        image = filters[i]\n",
    "\n",
    "        if normalize:\n",
    "            image = normalize_image(image)\n",
    "\n",
    "        ax = fig.add_subplot(rows, cols, i+1)\n",
    "        ax.imshow(image.permute(1, 2, 0))\n",
    "        ax.axis('off')\n",
    "        \n",
    "    fig.subplots_adjust(wspace = -0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gAcFeTR3BkaQ"
   },
   "source": [
    "There are some interesting patterns contained in these filters, however all of these were already present in the pre-trained ResNet model. The learning rate used on this initial convolutional layer was most probably too small to change these significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 846
    },
    "colab_type": "code",
    "id": "vqru5kYNxVNm",
    "outputId": "cb0d8899-2995-4b54-9eaf-65926c5d5d0a"
   },
   "outputs": [],
   "source": [
    "plot_filters(filters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lKKLTcF3BkaS"
   },
   "source": [
    "### Conclusions\n",
    "\n",
    "In this notebook we have shown: \n",
    "- downloading and extracting custom datasets\n",
    "- loading custom datasets\n",
    "- calculating the mean and std for normalization on custom datasets\n",
    "- loading transforms to augment and normalize our data\n",
    "- defining a ResNet model\n",
    "- defining the ResNet blocks\n",
    "- defining a CIFAR ResNet model\n",
    "- loading a pre-trained model\n",
    "- loading pre-trained model parameters into a defined model\n",
    "- how to use the learning rate finder\n",
    "- how to use discriminative fine-tuning\n",
    "- how to use the one cycle learning rate scheduler\n",
    "- fine-tuning a pre-trained model to achieve ~80% top-1 accuracy and ~95% top-5 accuracy on a dataset with 200 classes and only 60 examples per class\n",
    "- viewing our model's mistakes\n",
    "- visualizing our data in lower dimensions with PCA and t-SNE\n",
    "- viewing the learned weights of our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Free GPU memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    " \n",
    "pid = os.getpid()\n",
    "!kill -9 $pid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "5 - ResNet.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "2bfc54026555415b819aef84868882a0": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "33af98c97e6141f3bfc08f9cedb4dbd0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6237aa6a8e1843789e503390f5dcbd0d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "73733f1d2af0455890e9d6f695321d89": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9e26aa8e760c41c4bcd81d121abb8a54": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2bfc54026555415b819aef84868882a0",
      "max": 102502400,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_dcd14c4f39ba40b1924df3921551a759",
      "value": 102502400
     }
    },
    "d1868678a8d2487a85394a7eb6d85608": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6237aa6a8e1843789e503390f5dcbd0d",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_33af98c97e6141f3bfc08f9cedb4dbd0",
      "value": " 97.8M/97.8M [00:00&lt;00:00, 114MB/s]"
     }
    },
    "dcd14c4f39ba40b1924df3921551a759": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "e0d5f6ba52604352a6484d1907385629": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9e26aa8e760c41c4bcd81d121abb8a54",
       "IPY_MODEL_d1868678a8d2487a85394a7eb6d85608"
      ],
      "layout": "IPY_MODEL_73733f1d2af0455890e9d6f695321d89"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
