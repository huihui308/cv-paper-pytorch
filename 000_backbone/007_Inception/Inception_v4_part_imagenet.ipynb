{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# InceptionV4\n",
    "\n",
    "In this notebook we'll be implementing one of the [InceptionV4](https://arxiv.org/abs/1602.07261) model variants. InceptionV4 was designed for the [ImageNet challenge](http://www.image-net.org/challenges/LSVRC/), which it won in 2016.\n",
    "\n",
    "[Reference](https://www.geeksforgeeks.org/inception-v4-and-inception-resnets/)\n",
    "\n",
    "Inception V4 was introduced in combination with Inception-ResNet by the researchers a Google in 2016. The main aim of the paper was to reduce the complexity of Inception V3 model which give the state-of-the-art accuracy on ILSVRC 2015 challenge. This paper also explores the possibility of using residual networks on Inception model. This model \n",
    "\n",
    "## Architectural Changes in Inception-V4:\n",
    "\n",
    "In the paper there are two types of Inception architectures were discussed.\n",
    "\n",
    "- Pure Inception architecture (Inception -V4):\n",
    "    - The initial set of layers which the paper refers “stem of the architecture” was modified to make it more uniform . These layers are used before Inception block in the architecture.\n",
    "    - This model can be trained without partition of replicas unlike the previous versions of inceptions which required different replica in order to fit in memory. This architecture use memory optimization on back propagation to reduce the memory requirement.\n",
    "\n",
    "- Inception architecture with residuals:\n",
    "    - The authors of the paper was inspired by the success of Residual Network. Therefore they explored the possibility of combining the Inception with ResNets. They proposed two Residual Network based Inception models: Inception ResNet V1 and Inception ResNet V2. Let’s look at the key highlights of these architectures.\n",
    "        <div style=\"text-align:center\">\n",
    "            <img src=\"./assets/Inception-ResNet-connection.png\" width=\"60%\">\n",
    "        </div>\n",
    "\n",
    "- The Inception block used in these architecture are computationally less expensive than original Inception blocks that we used in Inception V4.\n",
    "\n",
    "- Each Inception block is followed by a 1×1 convolution without activation called filter expansion. This is done to scale up the dimensionality of filter bank to match the depth of input to next layer.\n",
    "\n",
    "- The pooling operation inside the Inception blocks were replaced by residual connections. However, pooling operations can be found in reduction blocks. \n",
    "\n",
    "- In Inception ResNets models, the batch normalization does not used after summations. This is done to reduce the model size to make it trainable on a single GPU.\n",
    "\n",
    "- Both the Inception architectures have same architectures for Reduction Blocks, but have different stem of the architectures. They also have difference in their hyper parameters for training.\n",
    "\n",
    "- It is found that Inception-ResNet V1 have similar computational cost as of Inception V3 and Inception-ResNet V2 have similar computational cost as of Inception V4.\n",
    "\n",
    "\n",
    "## Data Processing\n",
    "\n",
    "As always, we'll start by importing all the necessary modules. We have a few new imports here:\n",
    "- `lr_scheduler` for using the one cycle learning rate scheduler\n",
    "- `os` and `shutil` for handling custom datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchsummaryX import summary\n",
    "from tensorboardX import SummaryWriter\n",
    "import torchsummary\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "import torch.utils.data as data\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision import models\n",
    "\n",
    "from sklearn import decomposition\n",
    "from sklearn import manifold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from tqdm.notebook import tqdm, trange\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os, time, random, copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the random seed so all of our experiments can be reproduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show current directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "print(cwd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show torch version and if there is a gpu in this device. Also it is will print gpu device name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.__version__)\n",
    "\n",
    "device = (\"cuda\" if torch.cuda.is_available() else \"cpu\") # device\n",
    "print(device)\n",
    "\n",
    "print(\"The GPU device is:{}\".format( torch.cuda.get_device_name() ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architectures:\n",
    "\n",
    "- Below is the architectural details of Inception V4:\n",
    "    - Overall Architecture\n",
    "\n",
    "        <div style=\"text-align:center\">\n",
    "            <img src=\"./assets/Inception-V4.png\">\n",
    "        </div>\n",
    "\n",
    "First, we define basic class `BN_Conv2d`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BN_CONV_RELU\n",
    "class BN_Conv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, dilation=1, bias=True):\n",
    "        super(BN_Conv2d, self).__init__()\n",
    "        self.seq = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride,\n",
    "                      padding=padding, dilation=dilation, bias=bias),\n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.relu(self.seq(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inception4 modules\n",
    "\n",
    "<br>\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"./assets/InceptionV4-Inception-block.jpeg\" width=\"200%\" height=\"200%>\n",
    "</div>\n",
    "<div style=\"text-align:center\">Inception Modules A, B, C of Inception-v4</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inception_A(nn.Module):\n",
    "    \"\"\"\n",
    "    Inception-A block for Inception-v4 net\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, b1, b2, b3_n1, b3_n3, b4_n1, b4_n3):\n",
    "        super(Inception_A, self).__init__()\n",
    "        self.branch1 = nn.Sequential(\n",
    "            nn.AvgPool2d(3, 1, 1),\n",
    "            BN_Conv2d(in_channels, b1, 1, 1, 0, bias=False)\n",
    "        )\n",
    "        self.branch2 = BN_Conv2d(in_channels, b2, 1, 1, 0, bias=False)\n",
    "        self.branch3 = nn.Sequential(\n",
    "            BN_Conv2d(in_channels, b3_n1, 1, 1, 0, bias=False),\n",
    "            BN_Conv2d(b3_n1, b3_n3, 3, 1, 1, bias=False)\n",
    "        )\n",
    "        self.branch4 = nn.Sequential(\n",
    "            BN_Conv2d(in_channels, b4_n1, 1, 1, 0, bias=False),\n",
    "            BN_Conv2d(b4_n1, b4_n3, 3, 1, 1, bias=False),\n",
    "            BN_Conv2d(b4_n3, b4_n3, 3, 1, 1, bias=False)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out1 = self.branch1(x)\n",
    "        out2 = self.branch2(x)\n",
    "        out3 = self.branch3(x)\n",
    "        out4 = self.branch4(x)\n",
    "        return torch.cat((out1, out2, out3, out4), 1)\n",
    "\n",
    "\n",
    "class Inception_B(nn.Module):\n",
    "    \"\"\"\n",
    "    Inception-B block for Inception-v4 net\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, b1, b2, b3_n1, b3_n1x7, b3_n7x1, b4_n1, b4_n1x7_1,\n",
    "                 b4_n7x1_1, b4_n1x7_2, b4_n7x1_2):\n",
    "        super(Inception_B, self).__init__()\n",
    "        self.branch1 = nn.Sequential(\n",
    "            nn.AvgPool2d(3, 1, 1),\n",
    "            BN_Conv2d(in_channels, b1, 1, 1, 0, bias=False)\n",
    "        )\n",
    "        self.branch2 = BN_Conv2d(in_channels, b2, 1, 1, 0, bias=False)\n",
    "        self.branch3 = nn.Sequential(\n",
    "            BN_Conv2d(in_channels, b3_n1, 1, 1, 0, bias=False),\n",
    "            BN_Conv2d(b3_n1, b3_n1x7, (1, 7), (1, 1), (0, 3), bias=False),\n",
    "            BN_Conv2d(b3_n1x7, b3_n7x1, (7, 1), (1, 1), (3, 0), bias=False)\n",
    "        )\n",
    "        self.branch4 = nn.Sequential(\n",
    "            BN_Conv2d(in_channels, b4_n1, 1, 1, 0, bias=False),\n",
    "            BN_Conv2d(b4_n1, b4_n1x7_1, (1, 7), (1, 1), (0, 3), bias=False),\n",
    "            BN_Conv2d(b4_n1x7_1, b4_n7x1_1, (7, 1), (1, 1), (3, 0), bias=False),\n",
    "            BN_Conv2d(b4_n7x1_1, b4_n1x7_2, (1, 7), (1, 1), (0, 3), bias=False),\n",
    "            BN_Conv2d(b4_n1x7_2, b4_n7x1_2, (7, 1), (1, 1), (3, 0), bias=False)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out1 = self.branch1(x)\n",
    "        out2 = self.branch2(x)\n",
    "        out3 = self.branch3(x)\n",
    "        out4 = self.branch4(x)\n",
    "        return torch.cat((out1, out2, out3, out4), 1)\n",
    "\n",
    "\n",
    "class Inception_C(nn.Module):\n",
    "    \"\"\"\n",
    "    Inception-C block for Inception-v4 net\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, b1, b2, b3_n1, b3_n1x3_3x1, b4_n1,\n",
    "                 b4_n1x3, b4_n3x1, b4_n1x3_3x1):\n",
    "        super(Inception_C, self).__init__()\n",
    "        self.branch1 = nn.Sequential(\n",
    "            nn.AvgPool2d(3, 1, 1),\n",
    "            BN_Conv2d(in_channels, b1, 1, 1, 0, bias=False)\n",
    "        )\n",
    "        self.branch2 = BN_Conv2d(in_channels, b2, 1, 1, 0, bias=False)\n",
    "        self.branch3_1 = BN_Conv2d(in_channels, b3_n1, 1, 1, 0, bias=False)\n",
    "        self.branch3_1x3 = BN_Conv2d(b3_n1, b3_n1x3_3x1, (1, 3), (1, 1), (0, 1), bias=False)\n",
    "        self.branch3_3x1 = BN_Conv2d(b3_n1, b3_n1x3_3x1, (3, 1), (1, 1), (1, 0), bias=False)\n",
    "        self.branch4_1 = nn.Sequential(\n",
    "            BN_Conv2d(in_channels, b4_n1, 1, 1, 0, bias=False),\n",
    "            BN_Conv2d(b4_n1, b4_n1x3, (1, 3), (1, 1), (0, 1), bias=False),\n",
    "            BN_Conv2d(b4_n1x3, b4_n3x1, (3, 1), (1, 1), (1, 0), bias=False)\n",
    "        )\n",
    "        self.branch4_1x3 = BN_Conv2d(b4_n3x1, b4_n1x3_3x1, (1, 3), (1, 1), (0, 1), bias=False)\n",
    "        self.branch4_3x1 = BN_Conv2d(b4_n3x1, b4_n1x3_3x1, (3, 1), (1, 1), (1, 0), bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out1 = self.branch1(x)\n",
    "        out2 = self.branch2(x)\n",
    "        tmp = self.branch3_1(x)\n",
    "        out3_1 = self.branch3_1x3(tmp)\n",
    "        out3_2 = self.branch3_3x1(tmp)\n",
    "        tmp = self.branch4_1(x)\n",
    "        out4_1 = self.branch4_1x3(tmp)\n",
    "        out4_2 = self.branch4_3x1(tmp)\n",
    "        return torch.cat((out1, out2, out3_1, out3_2, out4_1, out4_2), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduction Modules\n",
    "\n",
    "<br>\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"./assets/Inception-V4-Reduction-Block.jpeg\" width=\"200%\" height=\"200%>\n",
    "</div>\n",
    "<div style=\"text-align:center\">Reduction Blocks A, B of Inception-v4</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reduction_A(nn.Module):\n",
    "    \"\"\"\n",
    "    Reduction-A block for Inception-v4, Inception-ResNet-v1, Inception-ResNet-v2 nets\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, k, l, m, n):\n",
    "        super(Reduction_A, self).__init__()\n",
    "        self.branch2 = BN_Conv2d(in_channels, n, 3, 2, 0, bias=False)\n",
    "        self.branch3 = nn.Sequential(\n",
    "            BN_Conv2d(in_channels, k, 1, 1, 0, bias=False),\n",
    "            BN_Conv2d(k, l, 3, 1, 1, bias=False),\n",
    "            BN_Conv2d(l, m, 3, 2, 0, bias=False)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out1 = F.max_pool2d(x, 3, 2, 0)\n",
    "        out2 = self.branch2(x)\n",
    "        out3 = self.branch3(x)\n",
    "        return torch.cat((out1, out2, out3), 1)\n",
    "\n",
    "\n",
    "class Reduction_B_v4(nn.Module):\n",
    "    \"\"\"\n",
    "    Reduction-B block for Inception-v4 net\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, b2_n1, b2_n3, b3_n1, b3_n1x7, b3_n7x1, b3_n3):\n",
    "        super(Reduction_B_v4, self).__init__()\n",
    "        self.branch2 = nn.Sequential(\n",
    "            BN_Conv2d(in_channels, b2_n1, 1, 1, 0, bias=False),\n",
    "            BN_Conv2d(b2_n1, b2_n3, 3, 2, 0, bias=False)\n",
    "        )\n",
    "        self.branch3 = nn.Sequential(\n",
    "            BN_Conv2d(in_channels, b3_n1, 1, 1, 0, bias=False),\n",
    "            BN_Conv2d(b3_n1, b3_n1x7, (1, 7), (1, 1), (0, 3), bias=False),\n",
    "            BN_Conv2d(b3_n1x7, b3_n7x1, (7, 1), (1, 1), (3, 0), bias=False),\n",
    "            BN_Conv2d(b3_n7x1, b3_n3, 3, 2, 0, bias=False)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out1 = F.max_pool2d(x, 3, 2, 0)\n",
    "        out2 = self.branch2(x)\n",
    "        out3 = self.branch3(x)\n",
    "        return torch.cat((out1, out2, out3), 1)\n",
    "\n",
    "\n",
    "class Reduction_B_Res(nn.Module):\n",
    "    \"\"\"\n",
    "    Reduction-B block for Inception-ResNet-v1 \\\n",
    "    and Inception-ResNet-v2  net\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, b2_n1, b2_n3, b3_n1, b3_n3, b4_n1, b4_n3_1, b4_n3_2):\n",
    "        super(Reduction_B_Res, self).__init__()\n",
    "        self.branch2 = nn.Sequential(\n",
    "            BN_Conv2d(in_channels, b2_n1, 1, 1, 0, bias=False),\n",
    "            BN_Conv2d(b2_n1, b2_n3, 3, 2, 0, bias=False),\n",
    "        )\n",
    "        self.branch3 = nn.Sequential(\n",
    "            BN_Conv2d(in_channels, b3_n1, 1, 1, 0, bias=False),\n",
    "            BN_Conv2d(b3_n1, b3_n3, 3, 2, 0, bias=False)\n",
    "        )\n",
    "        self.branch4 = nn.Sequential(\n",
    "            BN_Conv2d(in_channels, b4_n1, 1, 1, 0, bias=False),\n",
    "            BN_Conv2d(b4_n1, b4_n3_1, 3, 1, 1, bias=False),\n",
    "            BN_Conv2d(b4_n3_1, b4_n3_2, 3, 2, 0, bias=False)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out1 = F.max_pool2d(x, 3, 2, 0)\n",
    "        out2 = self.branch2(x)\n",
    "        out3 = self.branch3(x)\n",
    "        out4 = self.branch4(x)\n",
    "        return torch.cat((out1, out2, out3, out4), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Below is the architectural details of **Inception ResNet V1 and Inception ResNet V2** :\n",
    "    - Overall Architectures: Inception ResNet V2 has similar architecture schema as of V1 but the difference lies in their stems,  Inception and Reduction blocks.\n",
    "    \n",
    "        <div style=\"text-align:center\">\n",
    "            <img src=\"./assets/Inception-ResNet.png\">\n",
    "        </div>\n",
    "        <div style=\"text-align:center\">Inception ResNet V1 and Inception ResNet V2</div>\n",
    "       \n",
    "- Stem of the architecture\n",
    "\n",
    "    <div style=\"text-align:center\">\n",
    "        <img src=\"./assets/Inception-ResNet-V1-stem.png\">\n",
    "    </div>\n",
    "    <div style=\"text-align:center\">Inception ResNet v1 stem</div>\n",
    "    <br>\n",
    " \n",
    "    <div style=\"text-align:center\">\n",
    "        <img src=\"./assets/stem-InceptionV4-and-Inception-ResNet-V2.png\">\n",
    "    </div>\n",
    "    <div style=\"text-align:center\">Inception ResNet V2 stem</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stem_v4_Res2(nn.Module):\n",
    "    \"\"\"\n",
    "    stem block for Inception-v4 and Inception-RestNet-v2\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Stem_v4_Res2, self).__init__()\n",
    "        self.step1 = nn.Sequential(\n",
    "            BN_Conv2d(3, 32, 3, 2, 0, bias=False),\n",
    "            BN_Conv2d(32, 32, 3, 1, 0, bias=False),\n",
    "            BN_Conv2d(32, 64, 3, 1, 1, bias=False)\n",
    "        )\n",
    "        self.step2_pool = nn.MaxPool2d(3, 2, 0)\n",
    "        self.step2_conv = BN_Conv2d(64, 96, 3, 2, 0, bias=False)\n",
    "        self.step3_1 = nn.Sequential(\n",
    "            BN_Conv2d(160, 64, 1, 1, 0, bias=False),\n",
    "            BN_Conv2d(64, 96, 3, 1, 0, bias=False)\n",
    "        )\n",
    "        self.step3_2 = nn.Sequential(\n",
    "            BN_Conv2d(160, 64, 1, 1, 0, bias=False),\n",
    "            BN_Conv2d(64, 64, (7, 1), (1, 1), (3, 0), bias=False),\n",
    "            BN_Conv2d(64, 64, (1, 7), (1, 1), (0, 3), bias=False),\n",
    "            BN_Conv2d(64, 96, 3, 1, 0, bias=False)\n",
    "        )\n",
    "        self.step4_pool = nn.MaxPool2d(3, 2, 0)\n",
    "        self.step4_conv = BN_Conv2d(192, 192, 3, 2, 0, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.step1(x)\n",
    "        tmp1 = self.step2_pool(out)\n",
    "        tmp2 = self.step2_conv(out)\n",
    "        out = torch.cat((tmp1, tmp2), 1)\n",
    "        tmp1 = self.step3_1(out)\n",
    "        tmp2 = self.step3_2(out)\n",
    "        out = torch.cat((tmp1, tmp2), 1)\n",
    "        tmp1 = self.step4_pool(out)\n",
    "        tmp2 = self.step4_conv(out)\n",
    "        #print(tmp1.shape)\n",
    "        #print(tmp2.shape)\n",
    "        out = torch.cat((tmp1, tmp2), 1)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Stem_Res1(nn.Module):\n",
    "    \"\"\"\n",
    "    stem block for Inception-ResNet-v1\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Stem_Res1, self).__init__()\n",
    "        self.stem = nn.Sequential(\n",
    "            BN_Conv2d(3, 32, 3, 2, 0, bias=False),\n",
    "            BN_Conv2d(32, 32, 3, 1, 0, bias=False),\n",
    "            BN_Conv2d(32, 64, 3, 1, 1, bias=False),\n",
    "            nn.MaxPool2d(3, 2, 0),\n",
    "            BN_Conv2d(64, 80, 1, 1, 0, bias=False),\n",
    "            BN_Conv2d(80, 192, 3, 1, 0, bias=False),\n",
    "            BN_Conv2d(192, 256, 3, 2, 0, bias=False)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.stem(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inception Blocks: Inception blocks in Inception ResNets are very similar except for few changes in number of parameters. In Inception ResNet V2 the number of parameters increase in some layers in comparison to Inception ResNet V1.\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"./assets/Inception-ResNet-V1V2-Inception-Block.jpeg\">\n",
    "</div>\n",
    "<div style=\"text-align:center\">Inception modules A, B, C of Inception ResNet V1</div>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inception_A_res(nn.Module):\n",
    "    \"\"\"\n",
    "    Inception-A block for Inception-ResNet-v1\\\n",
    "    and Inception-ResNet-v2 net\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, b1, b2_n1, b2_n3, b3_n1, b3_n3_1, b3_n3_2, n1_linear):\n",
    "        super(Inception_A_res, self).__init__()\n",
    "        self.branch1 = BN_Conv2d(in_channels, b1, 1, 1, 0, bias=False)\n",
    "        self.branch2 = nn.Sequential(\n",
    "            BN_Conv2d(in_channels, b2_n1, 1, 1, 0, bias=False),\n",
    "            BN_Conv2d(b2_n1, b2_n3, 3, 1, 1, bias=False),\n",
    "        )\n",
    "        self.branch3 = nn.Sequential(\n",
    "            BN_Conv2d(in_channels, b3_n1, 1, 1, 0, bias=False),\n",
    "            BN_Conv2d(b3_n1, b3_n3_1, 3, 1, 1, bias=False),\n",
    "            BN_Conv2d(b3_n3_1, b3_n3_2, 3, 1, 1, bias=False)\n",
    "        )\n",
    "        self.conv_linear = nn.Conv2d(b1+b2_n3+b3_n3_2, n1_linear, 1, 1, 0, bias=True)\n",
    "\n",
    "        self.short_cut = nn.Sequential()\n",
    "        if in_channels != n1_linear:\n",
    "            self.short_cut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, n1_linear, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(n1_linear)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out1 = self.branch1(x)\n",
    "        out2 = self.branch2(x)\n",
    "        out3 = self.branch3(x)\n",
    "        out = torch.cat((out1, out2, out3), 1)\n",
    "        out = self.conv_linear(out)\n",
    "        out += self.short_cut(x)\n",
    "        return F.relu(out)\n",
    "\n",
    "\n",
    "class Inception_B_res(nn.Module):\n",
    "    \"\"\"\n",
    "    Inception-A block for Inception-ResNet-v1\\\n",
    "    and Inception-ResNet-v2 net\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, b1, b2_n1, b2_n1x7, b2_n7x1, n1_linear):\n",
    "        super(Inception_B_res, self).__init__()\n",
    "        self.branch1 = BN_Conv2d(in_channels, b1, 1, 1, 0, bias=False)\n",
    "        self.branch2 = nn.Sequential(\n",
    "            BN_Conv2d(in_channels, b2_n1, 1, 1, 0, bias=False),\n",
    "            BN_Conv2d(b2_n1, b2_n1x7, (1, 7), (1, 1), (0, 3), bias=False),\n",
    "            BN_Conv2d(b2_n1x7, b2_n7x1, (7, 1), (1, 1), (3, 0), bias=False)\n",
    "        )\n",
    "        self.conv_linear = nn.Conv2d(b1 + b2_n7x1, n1_linear, 1, 1, 0, bias=False)\n",
    "        self.short_cut = nn.Sequential()\n",
    "        if in_channels != n1_linear:\n",
    "            self.short_cut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, n1_linear, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(n1_linear)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out1 = self.branch1(x)\n",
    "        out2 = self.branch2(x)\n",
    "        out = torch.cat((out1, out2), 1)\n",
    "        out = self.conv_linear(out)\n",
    "        out += self.short_cut(x)\n",
    "        return F.relu(out)\n",
    "\n",
    "\n",
    "class Inception_C_res(nn.Module):\n",
    "    \"\"\"\n",
    "    Inception-C block for Inception-ResNet-v1\\\n",
    "    and Inception-ResNet-v2 net\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, b1, b2_n1, b2_n1x3, b2_n3x1, n1_linear):\n",
    "        super(Inception_C_res, self).__init__()\n",
    "        self.branch1 = BN_Conv2d(in_channels, b1, 1, 1, 0, bias=False)\n",
    "        self.branch2 = nn.Sequential(\n",
    "            BN_Conv2d(in_channels, b2_n1, 1, 1, 0, bias=False),\n",
    "            BN_Conv2d(b2_n1, b2_n1x3, (1, 3), (1, 1), (0, 1), bias=False),\n",
    "            BN_Conv2d(b2_n1x3, b2_n3x1, (3, 1), (1, 1), (1, 0), bias=False)\n",
    "        )\n",
    "        self.conv_linear = nn.Conv2d(b1 + b2_n3x1, n1_linear, 1, 1, 0, bias=False)\n",
    "        self.short_cut = nn.Sequential()\n",
    "        if in_channels != n1_linear:\n",
    "            self.short_cut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, n1_linear, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(n1_linear)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out1 = self.branch1(x)\n",
    "        out2 = self.branch2(x)\n",
    "        out = torch.cat((out1, out2), 1)\n",
    "        out = self.conv_linear(out)\n",
    "        out += self.short_cut(x)\n",
    "        return F.relu(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Reduction Blocks:\n",
    "\n",
    "    <div style=\"text-align:center\">\n",
    "        <img src=\"./assets/ReductionA.png\">\n",
    "    </div>\n",
    "    <div style=\"text-align:center\">Reduction A schema</div>\n",
    "    <br>\n",
    "\n",
    "- The reduction module A in different Inception architecture is similar. The only difference in number of parameters that are defined by table below:\n",
    "\n",
    "    <div style=\"text-align:center\">\n",
    "        <img src=\"./assets/ReductionAtable.png\">\n",
    "    </div>\n",
    "    <div style=\"text-align:center\">Hyper parameters of Inception-v4</div>\n",
    "    <br>\n",
    "\n",
    "- The Reduction Block B for Inception ResNets are given below:\n",
    "\n",
    "    <div style=\"text-align:center\">\n",
    "        <img src=\"./assets/RBIRV1.png\">\n",
    "    </div>\n",
    "    <div style=\"text-align:center\">Inception ResNet-v1 Reduction Block B</div>\n",
    "    <br>\n",
    "\n",
    "    <div style=\"text-align:center\">\n",
    "        <img src=\"./assets/ReductionBInceptionResNetV2.png\">\n",
    "    </div>\n",
    "    <div style=\"text-align:center\">Inception ResNet-v2 Reduction Block B</div>\n",
    "    <br>\n",
    "\n",
    "Define Inception model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inception(nn.Module):\n",
    "    \"\"\"\n",
    "    implementation of Inception-v4, Inception-ResNet-v1, Inception-ResNet-v2\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, version, num_classes):\n",
    "        super(Inception, self).__init__()\n",
    "        self.version = version\n",
    "        self.stem = Stem_Res1() if self.version == \"res1\" else Stem_v4_Res2()\n",
    "        self.inception_A = self.__make_inception_A()\n",
    "        self.Reduction_A = self.__make_reduction_A()\n",
    "        self.inception_B = self.__make_inception_B()\n",
    "        self.Reduction_B = self.__make_reduction_B()\n",
    "        self.inception_C = self.__make_inception_C()\n",
    "        if self.version == \"v4\":\n",
    "            self.fc = nn.Linear(1536, num_classes)\n",
    "        elif self.version == \"res1\":\n",
    "            self.fc = nn.Linear(1792, num_classes)\n",
    "        else:\n",
    "            self.fc = nn.Linear(2144, num_classes)\n",
    "\n",
    "        # Official init from torch repo.\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def __make_inception_A(self):\n",
    "        layers = []\n",
    "        if self.version == \"v4\":\n",
    "            for _ in range(4):\n",
    "                layers.append(Inception_A(384, 96, 96, 64, 96, 64, 96))\n",
    "        elif self.version == \"res1\":\n",
    "            for _ in range(5):\n",
    "                layers.append(Inception_A_res(256, 32, 32, 32, 32, 32, 32, 256))\n",
    "        else:\n",
    "            for _ in range(5):\n",
    "                layers.append(Inception_A_res(384, 32, 32, 32, 32, 48, 64, 384))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def __make_reduction_A(self):\n",
    "        if self.version == \"v4\":\n",
    "            return Reduction_A(384, 192, 224, 256, 384) # 1024\n",
    "        elif self.version == \"res1\":\n",
    "            return Reduction_A(256, 192, 192, 256, 384) # 896\n",
    "        else:\n",
    "            return Reduction_A(384, 256, 256, 384, 384) # 1152\n",
    "\n",
    "    def __make_inception_B(self):\n",
    "        layers = []\n",
    "        if self.version == \"v4\":\n",
    "            for _ in range(7):\n",
    "                layers.append(Inception_B(1024, 128, 384, 192, 224, 256,\n",
    "                                          192, 192, 224, 224, 256))   # 1024\n",
    "        elif self.version == \"res1\":\n",
    "            for _ in range(10):\n",
    "                layers.append(Inception_B_res(896, 128, 128, 128, 128, 896))  # 896\n",
    "        else:\n",
    "            for _ in range(10):\n",
    "                layers.append(Inception_B_res(1152, 192, 128, 160, 192, 1152))  # 1152\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def __make_reduction_B(self):\n",
    "        if self.version == \"v4\":\n",
    "            return Reduction_B_v4(1024, 192, 192, 256, 256, 320, 320)  # 1536\n",
    "        elif self.version == \"res1\":\n",
    "            return Reduction_B_Res(896, 256, 384, 256, 256, 256, 256, 256)  # 1792\n",
    "        else:\n",
    "            return Reduction_B_Res(1152, 256, 384, 256, 288, 256, 288, 320)  # 2144\n",
    "\n",
    "    def __make_inception_C(self):\n",
    "        layers = []\n",
    "        if self.version == \"v4\":\n",
    "            for _ in range(3):\n",
    "                layers.append(Inception_C(1536, 256, 256, 384, 256, 384, 448, 512, 256))\n",
    "        elif self.version == \"res1\":\n",
    "            for _ in range(5):\n",
    "                layers.append(Inception_C_res(1792, 192, 192, 192, 192, 1792))\n",
    "        else:\n",
    "            for _ in range(5):\n",
    "                layers.append(Inception_C_res(2144, 192, 192, 224, 256, 2144))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.stem(x)\n",
    "        out = self.inception_A(out)\n",
    "        out = self.Reduction_A(out)\n",
    "        out = self.inception_B(out)\n",
    "        out = self.Reduction_B(out)\n",
    "        out = self.inception_C(out)\n",
    "        out = F.avg_pool2d(out, 8)\n",
    "        out = F.dropout(out, 0.2, training=self.training)\n",
    "        h = out.view(out.size(0), -1)\n",
    "        #print(out.shape)\n",
    "        out = self.fc(h)\n",
    "        return (F.softmax(out, dim=1), h)\n",
    "\n",
    "\n",
    "def inception_v4(classes=1000):\n",
    "    return Inception(\"v4\", classes)\n",
    "\n",
    "\n",
    "def inception_resnet_v1(classes=1000):\n",
    "    return Inception(\"res1\", classes)\n",
    "\n",
    "\n",
    "def inception_resnet_v2(classes=1000):\n",
    "    return Inception(\"res2\", classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we'll see how many trainable parameters our model has. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll create an instance of our model with the desired amount of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIM = 20\n",
    "\n",
    "incept_v4_model = inception_v4(classes=20)\n",
    "print(f'Inception_v4 model has {count_parameters(incept_v4_model):,} trainable parameters')\n",
    "print(\"\\n\")\n",
    "torchsummary.summary(incept_v4_model, input_size=(3, 299, 299), batch_size=1, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIM = 20\n",
    "\n",
    "incept_rest_v1_model = inception_resnet_v1(classes=OUTPUT_DIM)\n",
    "print(f'Inception resnet v1 model has {count_parameters(incept_rest_v1_model):,} trainable parameters')\n",
    "print(\"\\n\")\n",
    "torchsummary.summary(incept_rest_v1_model, input_size=(3, 299, 299), batch_size=1, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIM = 20\n",
    "\n",
    "incept_rest_v2_model = inception_resnet_v2(classes=OUTPUT_DIM)\n",
    "print(f'Inception resnet v2 model has {count_parameters(incept_rest_v2_model):,} trainable parameters')\n",
    "print(\"\\n\")\n",
    "torchsummary.summary(incept_rest_v2_model, input_size=(3, 299, 299), batch_size=1, device='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare datasets\n",
    "\n",
    "Because of Imagenet is very large, we only use part of it data, only 20 classes. Make a `dataset` directory in current dir and soft link ILSVRC2012 to dataset directory.\n",
    "```shell\n",
    "$ mkdir -p dataset/ILSVRC2012/train\n",
    "$ mkdir -p dataset/ILSVRC2012/val\n",
    "$ cd dataset/ILSVRC2012/train/\n",
    "$ ln -s ./../../../../data/ILSVRC2012/train/n022* ./\n",
    "$ ln -s ./../../../../data/ILSVRC2012/train/n02391049 ./\n",
    "$ ln -s ./../../../../data/ILSVRC2012/train/n02396427 ./\n",
    "$ cd ./../val\n",
    "$ ln -s ./../../../../data/ILSVRC2012/val/n022* ./\n",
    "$ ln -s ./../../../../data/ILSVRC2012/val/n02391049 ./\n",
    "$ ln -s ./../../../../data/ILSVRC2012/val/n02396427 ./\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_img_train = \"./dataset/ILSVRC2012/train/\"\n",
    "path_img_val = \"./dataset/ILSVRC2012/val/\"\n",
    "\n",
    "path_log = \"./tblog/\"\n",
    "path_checkpoint = \"./checkpoints/\"\n",
    "if not os.path.exists(path_log):\n",
    "    os.makedirs(path_log)\n",
    "if not os.path.exists(path_checkpoint):\n",
    "    os.makedirs(path_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll preprocess dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tbwriter = SummaryWriter(log_dir=path_log)\n",
    "\n",
    "num_epochs = 100\n",
    "batch_size = 64\n",
    "cropsize = 299"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = datasets.ImageFolder(path_img_train, transforms.Compose([\n",
    "    transforms.Resize(cropsize),\n",
    "    transforms.RandomCrop(cropsize),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor()\n",
    "]))\n",
    "dataset_val = datasets.ImageFolder(path_img_val, transforms.Compose([\n",
    "    transforms.Resize(cropsize),\n",
    "    transforms.RandomCrop(cropsize),\n",
    "    transforms.ToTensor()\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator = DataLoader(\n",
    "    dataset=dataset_train,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=6,\n",
    "    pin_memory=True,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "valid_iterator = DataLoader(\n",
    "    dataset=dataset_val,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=6,\n",
    "    pin_memory=True,\n",
    "    drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = incept_rest_v1_model\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create a new optimizer with our found learning rate.\n",
    "\n",
    "Ironically, the learning rate value we found, $0.001$ is actually Adam's default learning rate!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOUND_LR = 1e-3\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=FOUND_LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest of the notebook is pretty similar to the previous notebooks from these tutorials.\n",
    "\n",
    "We define a function to calculate accuracy..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(y_pred, y):\n",
    "    top_pred = y_pred.argmax(1, keepdim=True)\n",
    "    correct = top_pred.eq(y.view_as(top_pred)).sum()\n",
    "    acc = correct.float() / y.shape[0]\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and a function to implement our training loop.\n",
    "\n",
    "As we are using dropout we need to make sure to \"turn it on\" when training by using `model.train()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, device):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    top3_train_accuracy = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for (x, y) in tqdm(iterator, desc=\"Training\", leave=False):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        y_pred, _ = model(x)\n",
    "\n",
    "        loss = criterion(y_pred, y)\n",
    "\n",
    "        acc = calculate_accuracy(y_pred, y)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "        ps = torch.exp(y_pred)\n",
    "        np_top3_class = ps.topk(3, dim=1)[1].cpu().numpy()\n",
    "        target_numpy = y.cpu().numpy()\n",
    "        top3_train_accuracy += np.mean([1 if target_numpy[i] in np_top3_class[i] else 0 for i in range(0, len(target_numpy))])\n",
    "\n",
    "    return (epoch_loss/len(iterator)), (epoch_acc/len(iterator)), (top3_train_accuracy/len(iterator))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also define an evaluation loop, making sure to \"turn off\" dropout with `model.eval()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion, device):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    top3_test_accuracy = 0\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for (x, y) in tqdm(iterator, desc=\"Evaluating\", leave=False):\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            y_pred, _ = model(x)\n",
    "\n",
    "            loss = criterion(y_pred, y)\n",
    "\n",
    "            acc = calculate_accuracy(y_pred, y)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "            \n",
    "            ps = torch.exp(y_pred)\n",
    "            np_top3_class = ps.topk(3, dim=1)[1].cpu().numpy()\n",
    "            target_numpy = y.cpu().numpy()\n",
    "            top3_test_accuracy += np.mean([1 if target_numpy[i] in np_top3_class[i] else 0 for i in range(0, len(target_numpy))])\n",
    "            \n",
    "    return (epoch_loss/len(iterator)), (epoch_acc/len(iterator)), (top3_test_accuracy/len(iterator))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define a function to tell us how long an epoch takes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return (elapsed_mins, elapsed_secs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, finally, we train our model.\n",
    "\n",
    "We get a best validation loss of ~88% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "train_stats = pd.DataFrame(columns = ['Epoch', 'Time per epoch', 'Avg time per step', \n",
    "                                      'Train loss', 'Train accuracy', 'Train top-3 accuracy', \n",
    "                                      'Test loss', 'Test accuracy', 'Test top-3 accuracy'])\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in trange(EPOCHS, desc=\"Epochs\"):\n",
    "    start_time = time.monotonic()\n",
    "\n",
    "    train_loss, train_acc, top3_train_acc = train(model, train_iterator, optimizer, criterion, device)\n",
    "    valid_loss, valid_acc, top3_valid_acc = evaluate(model, valid_iterator, criterion, device)\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut3-model.pt')\n",
    "\n",
    "    end_time = time.monotonic()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    time_elapsed = end_time - start_time\n",
    "\n",
    "    df = pd.DataFrame.from_dict(\n",
    "                [ {'Epoch': epoch, 'Time per epoch':time_elapsed, \n",
    "                  'Avg time per step': time_elapsed/len(train_iterator), \n",
    "                  'Train loss' : train_loss, \n",
    "                  'Train accuracy': train_acc, \n",
    "                  'Train top-3 accuracy':top3_train_acc,\n",
    "                  'Test loss' : valid_loss, \n",
    "                  'Test accuracy': valid_acc, \n",
    "                  'Test top-3 accuracy':top3_valid_acc} ] )\n",
    "    train_stats = pd.concat([ train_stats, df], ignore_index=True)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}% | Top3 train Acc: {train_acc*100:.2f}%')  \n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}% | Top3 valid Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Free cuda memory："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Free GPU memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    " \n",
    "pid = os.getpid()\n",
    "!kill -9 $pid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End......"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
