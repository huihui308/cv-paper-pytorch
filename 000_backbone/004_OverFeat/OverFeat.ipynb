{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OverFeat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we'll be implementing one of the [OverFeat](https://arxiv.org/abs/1312.6229) (OverFeat Network) model variants. OverFeat was designed for the [ImageNet challenge](http://www.image-net.org/challenges/LSVRC/), which it won in 2013.\n",
    "\n",
    "We present an integrated framework for using Convolutional Networks for classification, localization and detection. We show how a multiscale and sliding window approach can be efficiently implemented within a ConvNet. We also introduce a novel deep learning approach to localization by learning to predict object boundaries. Bounding boxes are then accumulated rather than suppressed in order to increase detection confidence. We show that different tasks can be learned simultaneously using a single shared network. This integrated framework is the winner of the localization task of the ImageNet Large Scale Visual Recognition Challenge 2013 (ILSVRC2013) and obtained very competitive results for the detection and classifications tasks. In post-competition work, we establish a new state of the art for the detection task. Finally, we release a feature extractor from our best model called OverFeat.\n",
    "\n",
    "OverFeat is a classic type of convolutional neural network architecture, employing convolution, pooling and fully connected layers. The Figure to the right shows the architectural details. Source: OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks.\n",
    "\n",
    "### Data Processing\n",
    "\n",
    "As always, we'll start by importing all the necessary modules. We have a few new imports here:\n",
    "- `lr_scheduler` for using the one cycle learning rate scheduler\n",
    "- `os` and `shutil` for handling custom datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchsummaryX import summary\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "import torch.utils.data as data\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision import models\n",
    "\n",
    "from sklearn import decomposition\n",
    "from sklearn import manifold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from tqdm.notebook import tqdm, trange\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import copy\n",
    "import random\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the random seed so all of our experiments can be reproduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show current directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/david/code/paper/cv-paper-pytorch/004_OverFeat\n"
     ]
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "print(cwd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show torch version and if there is a gpu in this device. Also it is will print gpu device name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.1+cu118\n",
      "cuda\n",
      "The GPU device is:NVIDIA GeForce RTX 3080 Ti\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "\n",
    "device = (\"cuda\" if torch.cuda.is_available() else \"cpu\") # device\n",
    "print(device)\n",
    "\n",
    "print(\"The GPU device is:{}\".format( torch.cuda.get_device_name() ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Model\n",
    "\n",
    "Next up is defining the model.\n",
    "\n",
    "The actual model itself is no more difficult to understand than the previous model, LeNet. It is made up of convolutional layers, pooling layers and ReLU activation functions. See the previous notebook for a refresher on these concepts. \n",
    "\n",
    "There are only two new concepts introduced here, `nn.Sequential` and `nn.Dropout`.\n",
    "\n",
    "We can think of `Sequential` as like our transforms introduced earlier for data augmentation. We provide `Sequential` with multiple layers and when the `Sequential` module is called it will apply each layer, in order, to the input. There is no difference between using a `Sequential` and having each module defined in the `__init__` and then called in `forward` - however it makes the code significantly shorter.\n",
    "\n",
    "We have one `Sequential` model, `features`, for all the convolutional and pooling layers, then we flatten then data and pass it to the `classifier`, another `Sequential` model which is made up of linear layers and the second new concept, *dropout*.\n",
    "\n",
    "Dropout is a form of [*regularization*](https://en.wikipedia.org/wiki/Regularization_(mathematics)). As our models get larger, to perform more accurately on richer datasets, they start having a significantly higher number of parameters. The problem with lots of parameters is that our models begin to *overfit*. That is, they do not learn general image features whilst learning to classify images but instead simply memorize images within the training set. This is bad as it will cause our model to achieve poor performance on the validation/testing set. To solve this overfitting problem, we use regularization. Dropout is just one method of regularization, other common ones are *L1 regularization*, *L2 regularization* and *weight decay*.\n",
    "\n",
    "Dropout works by randomly setting a certain fraction, 0.5 here, of the neurons in a layer to zero. This effectively adds noise to the training of the neural network and causes neurons to learn with \"less\" data as they are only getting half of the information from a previous layer with dropout applied. It can also be thought of as causing your model to learn multiple smaller models with less parameters. \n",
    "\n",
    "Dropout is only applied when the model is training. It needs to be \"turned off\" when validating, testing or using the model for inference.\n",
    "\n",
    "As mentioned in the previous notebook, during the convolutional and pooling layers the activation function should be placed **after** the pooling layer to reduce computational cost.\n",
    "\n",
    "In the linear layers, dropout should be applied **after** the activation function. Although when using ReLU activation functions the same result is achieved if dropout is before or after, see [here](https://sebastianraschka.com/faq/docs/dropout-activation.html).\n",
    "\n",
    "One last thing to mention is that the very first convolutional layer has an `in_channel` of three. That is because we are handling color images that have three channels (red, green and blue) instead of the single channel grayscale images from the MNIST dataset. This doesn't change the way any of the convolutional filter works, it just means the first filter has a depth of three instead of a depth of one.\n",
    "\n",
    "The accurate model for overfeat is as following.\n",
    "![](./assets/accurate_overfeat.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OverFeat_accurate(nn.Module):\n",
    "    def __init__(self, num_classes=1000):\n",
    "        super().__init__()\n",
    "\n",
    "        # train with 221x221 5 random crops and their horizontal filps\n",
    "        # mini- batches of size 128\n",
    "        # initialized weight randomly with mu=0, sigma=1x10^-2\n",
    "        # SGD, momentum=0.6, l2 weight decay of 1x10^-5\n",
    "        # learning rate 5x10^-2, decay by 0.5 after (30, 50, 60, 70, 80) epochs\n",
    "        # Dropout on FCN?? -> dropout before classifier conv layer\n",
    "\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            # no contrast normalization is used\n",
    "            # max polling with non-overlapping\n",
    "            # 1st and 2nd layer stride 2 instead of 4\n",
    "\n",
    "            # 1st\n",
    "            nn.Conv2d(in_channels=3, out_channels=96, kernel_size=7, stride=2),  # (b x 96 x 108 x 108)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=3),  # (b x 96 x 36 x 36)\n",
    "\n",
    "            # 2nd\n",
    "            nn.Conv2d(96, 256, 7, stride= 1),  # (b x 256 x 30 x 30)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # (b x 256 x 15 x 15)\n",
    "\n",
    "            # 3rd\n",
    "            nn.Conv2d(256, 512, 3, padding=1),  # (b x 512 x 15 x 15)\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # 4th\n",
    "            nn.Conv2d(512, 512, 3, padding=1),  # (b x 512 x 15 x 15)\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # 5th\n",
    "            nn.Conv2d(512, 1024, 3, padding=1),  # (b x 1024 x 15 x 15)\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # 6th\n",
    "            nn.Conv2d(1024, 1024, 3, padding=1),  # (b x 1024 x 15 x 15)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=3),  # (b x 1024 x 5 x 5)\n",
    "        )\n",
    "\n",
    "        # fully connecyed layers implemented as a convolution layers\n",
    "        self.classifier = nn.Sequential(\n",
    "            # 7th\n",
    "            nn.Dropout(p=0.5, inplace=True),\n",
    "            nn.Conv2d(in_channels=1024, out_channels=4096, kernel_size=5),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # 8th\n",
    "            nn.Dropout(p=0.5, inplace=True),\n",
    "            nn.Conv2d(4096, 4096, 1),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # 9th\n",
    "            nn.Conv2d(4096, num_classes, 1)\n",
    "        )\n",
    "\n",
    "        self.init_weight()  # initialize weight\n",
    "\n",
    "    def init_weight(self):\n",
    "        for layer in self.feature_extractor:\n",
    "            if isinstance(layer, nn.Conv2d):\n",
    "                nn.init.normal_(layer.weight, mean=0, std=0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Pass the input through the net.\n",
    "        Args:\n",
    "            x (Tensor): input tensor\n",
    "        Returns:\n",
    "            output (Tensor): output tensor\n",
    "        \"\"\"\n",
    "        x = self.feature_extractor(x)\n",
    "        return self.classifier(x).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fast model for overfeat is as following.\n",
    "![](./assets/fast_overfeat.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OverFeat_fast(nn.Module):\n",
    "    def __init__(self, num_classes=1000):\n",
    "        super().__init__()\n",
    "\n",
    "        # train with 221x221 5 random crops and their horizontal filps\n",
    "        # mini- batches of size 128\n",
    "        # initialized weight randomly with mu=0, sigma=1x10^-2\n",
    "        # SGD, momentum=0.6, l2 weight decay of 1x10^-5\n",
    "        # learning rate 5x10^-2, decay by 0.5 after (30, 50, 60, 70, 80) epochs\n",
    "        # Dropout on FCN?? -> dropout before classifier conv layer\n",
    "\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            # no contrast normalization is used\n",
    "            # max polling with non-overlapping\n",
    "            # 1st and 2nd layer stride 2 instead of 4\n",
    "\n",
    "            # 1st\n",
    "            nn.Conv2d(in_channels=3, out_channels=96, kernel_size=11, stride=4),  # (b x 96 x 56 x 56)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # (b x 96 x 28 x 28)\n",
    "\n",
    "            # 2nd\n",
    "            nn.Conv2d(96, 256, 5, stride= 1),  # (b x 256 x 24 x 24)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # (b x 256 x 12 x 12)\n",
    "\n",
    "            # 3rd\n",
    "            nn.Conv2d(256, 512, 3, padding=1),  # (b x 512 x 12 x 12)\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # 4th\n",
    "            nn.Conv2d(512, 1024, 3, padding=1),  # (b x 1024 x 12 x 12)\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # 5th\n",
    "            nn.Conv2d(1024, 1024, 3, padding=1),  # (b x 1024 x 12 x 12)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # (b x 1024 x 6 x 6)\n",
    "        )\n",
    "\n",
    "        # fully connecyed layers implemented as a convolution layers\n",
    "        self.classifier = nn.Sequential(\n",
    "            # 6th\n",
    "            nn.Dropout(p=0.5, inplace=False),\n",
    "            nn.Conv2d(in_channels=1024, out_channels=3072, kernel_size=6),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # 7th\n",
    "            nn.Dropout(p=0.5, inplace=False),\n",
    "            nn.Conv2d(3072, 4096, 1),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # 8th\n",
    "            nn.Conv2d(4096, num_classes, 1)\n",
    "        )\n",
    "\n",
    "        self.init_weight()  # initialize weight\n",
    "\n",
    "    def init_weight(self):\n",
    "        for layer in self.feature_extractor:\n",
    "            if isinstance(layer, nn.Conv2d):\n",
    "                nn.init.normal_(layer.weight, mean=0, std=0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Pass the input through the net.\n",
    "        Args:\n",
    "            x (Tensor): input tensor\n",
    "        Returns:\n",
    "            output (Tensor): output tensor\n",
    "        \"\"\"\n",
    "        x = self.feature_extractor(x)\n",
    "        return self.classifier(x).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll create an instance of our model with the desired amount of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fast = True\n",
    "OUTPUT_DIM = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Fast == True:\n",
    "    model = OverFeat_fast(num_classes=OUTPUT_DIM)\n",
    "    #summary(overfeat, torch.zeros((128,3,231,231),device=device))\n",
    "else:\n",
    "    model = OverFeat_accurate(num_classes=1000).to(device)\n",
    "    #summary(overfeat, torch.zeros((128,3,221,221),device=device))\n",
    "\n",
    "# overfeat = torch.nn.parallel.DataParallel(overfeat, device_ids=)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we'll see how many trainable parameters our model has. \n",
    "\n",
    "Our LeNet architecture had ~44k, but here we have 141.9M parameters - and AlexNet is a relatively small model for computer vision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 145,920,872 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put ILSVRC2012 `ILSVRC2012_img_train.tar` and `ILSVRC2012_img_val.tar` to `./../data` directory and untar them.\n",
    "\n",
    "Cd `./../data` and execute `./extract_ILSVRC2012.sh` to extract ILSVRC2012 data.\n",
    "\n",
    "And then rename all images suffix name.\n",
    "```shell\n",
    "find ./ -name \"*.JPEG\" | awk -F \".\" '{print $2}' | xargs -i -t mv ./{}.JPEG ./{}.jpg\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_img_train = \"./../data/ILSVRC2012/train/\"\n",
    "path_img_val = \"./../data/ILSVRC2012/val/\"\n",
    "\n",
    "path_log = \"./tblog/\"\n",
    "path_checkpoint = \"./checkpoints/\"\n",
    "if not os.path.exists(path_log):\n",
    "    os.makedirs(path_log)\n",
    "if not os.path.exists(path_checkpoint):\n",
    "    os.makedirs(path_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll set the random seeds for reproducability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tbwriter = SummaryWriter(log_dir=path_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fast = True\n",
    "\n",
    "num_epochs = 100\n",
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================================================================\n",
      "                                         Kernel Shape         Output Shape  \\\n",
      "Layer                                                                        \n",
      "0_feature_extractor.Conv2d_0          [3, 96, 11, 11]    [128, 96, 56, 56]   \n",
      "1_feature_extractor.ReLU_1                          -    [128, 96, 56, 56]   \n",
      "2_feature_extractor.MaxPool2d_2                     -    [128, 96, 28, 28]   \n",
      "3_feature_extractor.Conv2d_3          [96, 256, 5, 5]   [128, 256, 24, 24]   \n",
      "4_feature_extractor.ReLU_4                          -   [128, 256, 24, 24]   \n",
      "5_feature_extractor.MaxPool2d_5                     -   [128, 256, 12, 12]   \n",
      "6_feature_extractor.Conv2d_6         [256, 512, 3, 3]   [128, 512, 12, 12]   \n",
      "7_feature_extractor.ReLU_7                          -   [128, 512, 12, 12]   \n",
      "8_feature_extractor.Conv2d_8        [512, 1024, 3, 3]  [128, 1024, 12, 12]   \n",
      "9_feature_extractor.ReLU_9                          -  [128, 1024, 12, 12]   \n",
      "10_feature_extractor.Conv2d_10     [1024, 1024, 3, 3]  [128, 1024, 12, 12]   \n",
      "11_feature_extractor.ReLU_11                        -  [128, 1024, 12, 12]   \n",
      "12_feature_extractor.MaxPool2d_12                   -    [128, 1024, 6, 6]   \n",
      "13_classifier.Dropout_0                             -    [128, 1024, 6, 6]   \n",
      "14_classifier.Conv2d_1             [1024, 3072, 6, 6]    [128, 3072, 1, 1]   \n",
      "15_classifier.ReLU_2                                -    [128, 3072, 1, 1]   \n",
      "16_classifier.Dropout_3                             -    [128, 3072, 1, 1]   \n",
      "17_classifier.Conv2d_4             [3072, 4096, 1, 1]    [128, 4096, 1, 1]   \n",
      "18_classifier.ReLU_5                                -    [128, 4096, 1, 1]   \n",
      "19_classifier.Conv2d_6             [4096, 1000, 1, 1]    [128, 1000, 1, 1]   \n",
      "\n",
      "                                       Params     Mult-Adds  \n",
      "Layer                                                        \n",
      "0_feature_extractor.Conv2d_0          34.944k   109.283328M  \n",
      "1_feature_extractor.ReLU_1                  -             -  \n",
      "2_feature_extractor.MaxPool2d_2             -             -  \n",
      "3_feature_extractor.Conv2d_3         614.656k     353.8944M  \n",
      "4_feature_extractor.ReLU_4                  -             -  \n",
      "5_feature_extractor.MaxPool2d_5             -             -  \n",
      "6_feature_extractor.Conv2d_6         1.18016M   169.869312M  \n",
      "7_feature_extractor.ReLU_7                  -             -  \n",
      "8_feature_extractor.Conv2d_8        4.719616M   679.477248M  \n",
      "9_feature_extractor.ReLU_9                  -             -  \n",
      "10_feature_extractor.Conv2d_10      9.438208M  1.358954496G  \n",
      "11_feature_extractor.ReLU_11                -             -  \n",
      "12_feature_extractor.MaxPool2d_12           -             -  \n",
      "13_classifier.Dropout_0                     -             -  \n",
      "14_classifier.Conv2d_1             113.24928M   113.246208M  \n",
      "15_classifier.ReLU_2                        -             -  \n",
      "16_classifier.Dropout_3                     -             -  \n",
      "17_classifier.Conv2d_4             12.587008M    12.582912M  \n",
      "18_classifier.ReLU_5                        -             -  \n",
      "19_classifier.Conv2d_6                 4.097M        4.096M  \n",
      "-----------------------------------------------------------------------------------------------------\n",
      "                            Totals\n",
      "Total params           145.920872M\n",
      "Trainable params       145.920872M\n",
      "Non-trainable params           0.0\n",
      "Mult-Adds             2.801403904G\n",
      "=====================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david/.local/lib/python3.8/site-packages/torchsummaryX/torchsummaryX.py:101: FutureWarning: The default value of numeric_only in DataFrame.sum is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
      "  df_sum = df.sum()\n"
     ]
    }
   ],
   "source": [
    "if Fast:\n",
    "    overfeat = OverFeat_fast(num_classes=1000).to(device)\n",
    "    summary(overfeat, torch.zeros((128,3,231,231),device=device))\n",
    "else:\n",
    "    overfeat = OverFeat_accurate(num_classes=1000).to(device)\n",
    "    summary(overfeat, torch.zeros((128,3,221,221),device=device))\n",
    "\n",
    "# overfeat = torch.nn.parallel.DataParallel(overfeat, device_ids=)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Fast:\n",
    "    cropsize = 231\n",
    "else:\n",
    "    cropsize = 221"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = datasets.ImageFolder(path_img_train, transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.RandomCrop(cropsize),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor()\n",
    "]))\n",
    "dataset_val = datasets.ImageFolder(path_img_val, transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.RandomCrop(cropsize),\n",
    "    transforms.ToTensor()\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_train = DataLoader(\n",
    "    dataset=dataset_train,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=6,\n",
    "    pin_memory=True,\n",
    "    drop_last=True\n",
    ")\n",
    "dataloader_val = DataLoader(\n",
    "    dataset=dataset_val,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=6,\n",
    "    pin_memory=True,\n",
    "    drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(\n",
    "    params=overfeat.parameters(),\n",
    "    momentum=0.6,\n",
    "    weight_decay=1e-5,\n",
    "    lr=5e-2\n",
    ")\n",
    "\n",
    "lr_scheduler = optim.lr_scheduler.MultiStepLR(optimizer=optimizer, milestones=[30, 50, 60, 70, 80], gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a4a740ca18e483381de81675e8cabe5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', style=ProgressStyle(description_width='initial'))…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Training', max=5004.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tStep: 1000 \tLoss: 6.9062 \tAcc: 0\n",
      "\tValidation Loss: 6.9079 \t Validation Acc: 50 / 195 (25.641027%)\n",
      "Epoch: 1 \tStep: 2000 \tLoss: 6.9087 \tAcc: 0\n",
      "\tValidation Loss: 6.9080 \t Validation Acc: 50 / 195 (25.641027%)\n",
      "Epoch: 1 \tStep: 3000 \tLoss: 6.9093 \tAcc: 0\n",
      "\tValidation Loss: 6.9079 \t Validation Acc: 70 / 195 (35.897437%)\n",
      "Epoch: 1 \tStep: 4000 \tLoss: 6.8134 \tAcc: 0\n",
      "\tValidation Loss: 6.7961 \t Validation Acc: 133 / 195 (68.205130%)\n",
      "Epoch: 1 \tStep: 5000 \tLoss: 6.6055 \tAcc: 0\n",
      "\tValidation Loss: 6.6680 \t Validation Acc: 213 / 195 (109.230769%)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Training', max=5004.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 \tStep: 6000 \tLoss: 6.4145 \tAcc: 4\n",
      "\tValidation Loss: 6.4628 \t Validation Acc: 518 / 195 (265.641046%)\n",
      "Epoch: 2 \tStep: 7000 \tLoss: 5.9640 \tAcc: 2\n",
      "\tValidation Loss: 6.0596 \t Validation Acc: 1460 / 195 (748.717976%)\n",
      "Epoch: 2 \tStep: 8000 \tLoss: 5.6343 \tAcc: 13\n",
      "\tValidation Loss: 5.6394 \t Validation Acc: 2501 / 195 (1282.564163%)\n",
      "Epoch: 2 \tStep: 9000 \tLoss: 5.4886 \tAcc: 14\n",
      "\tValidation Loss: 5.4463 \t Validation Acc: 3400 / 195 (1743.589783%)\n",
      "Epoch: 2 \tStep: 10000 \tLoss: 5.1454 \tAcc: 21\n",
      "\tValidation Loss: 5.0273 \t Validation Acc: 4803 / 195 (2463.076973%)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b9d9fab57794819885e45d4f3d0dad6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Training', max=5004.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 \tStep: 11000 \tLoss: 4.9313 \tAcc: 30\n",
      "\tValidation Loss: 4.8763 \t Validation Acc: 5781 / 195 (2964.615440%)\n",
      "Epoch: 3 \tStep: 12000 \tLoss: 4.7576 \tAcc: 29\n",
      "\tValidation Loss: 4.5724 \t Validation Acc: 7051 / 195 (3615.897369%)\n"
     ]
    }
   ],
   "source": [
    "overfeat.train()\n",
    "step = 1\n",
    "\n",
    "#for epoch in range(num_epochs):\n",
    "for epoch in tqdm(range(num_epochs), desc=\"Epoch\", leave=False):\n",
    "    #for imgs, classes in dataloader_train:\n",
    "    for (imgs, classes) in tqdm(dataloader_train, desc=\"Training\", leave=False):\n",
    "        imgs, classes = imgs.to(device), classes.to(device)\n",
    "\n",
    "        output = overfeat(imgs)\n",
    "        loss = F.cross_entropy(output, classes)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if(step % 1e3 == 0):\n",
    "            with torch.no_grad():\n",
    "                _, preds = torch.max(output, 1)\n",
    "                accuracy = torch.sum(preds == classes)\n",
    "\n",
    "                print(f'Epoch: {epoch + 1} \\tStep: {step} \\tLoss: {loss.item():.4f} \\tAcc: {accuracy.item()}')\n",
    "                tbwriter.add_scalar('loss', loss.item(), step)\n",
    "                tbwriter.add_scalar('accuracy', accuracy.item(), step)\n",
    "\n",
    "                for name, parameter in overfeat.named_parameters():\n",
    "                    if parameter.grad is not None:\n",
    "                        avg_grad = torch.mean(parameter.grad)\n",
    "                        # print(f'\\t{name} - grad_avg: {avg_grad}')\n",
    "                        tbwriter.add_scalar(f'grad_avg/{name}', avg_grad.item(), step)\n",
    "                        tbwriter.add_histogram(f'grad/{name}', parameter.grad.cpu().numpy(), step)\n",
    "                    if parameter.data is not None:\n",
    "                        avg_weight = torch.mean(parameter.data)\n",
    "                        # print(f'\\t{name} - param_avg: {avg_weight}')\n",
    "                        tbwriter.add_histogram(f'weight/{name}', parameter.data.cpu().numpy(), step)\n",
    "                        tbwriter.add_scalar(f'weight_avg/{name}', avg_weight.item(), step)\n",
    "\n",
    "                overfeat.eval()\n",
    "                val_cLoss = 0\n",
    "                val_cAcc = 0\n",
    "                val_count = 0\n",
    "                \n",
    "                for val_imgs, val_classes in dataloader_val:\n",
    "                    val_imgs, val_classes = val_imgs.to(device), val_classes.to(device)\n",
    "\n",
    "                    val_output = overfeat(val_imgs)\n",
    "                    val_cLoss += F.cross_entropy(val_output, val_classes)\n",
    "\n",
    "                    _, val_pred = torch.max(val_output, 1)\n",
    "                    val_cAcc += torch.sum(val_pred == val_classes)\n",
    "\n",
    "                    val_count += 1\n",
    "\n",
    "                val_loss = val_cLoss / val_count\n",
    "                val_accuracy = val_cAcc / val_count\n",
    "\n",
    "                print(f'\\tValidation Loss: {val_loss:.4f} \\t Validation Acc: {val_cAcc} / {val_count} ({val_accuracy.item()*100:0f}%)')\n",
    "                tbwriter.add_scalar('val_loss', val_loss.item(), step)\n",
    "                tbwriter.add_scalar('val_accuracy', val_accuracy.item(), step)\n",
    "                overfeat.train()\n",
    "\n",
    "        step += 1\n",
    "        \n",
    "    lr_scheduler.step()\n",
    "    \n",
    "    if(Fast):\n",
    "        checkpoint_path = os.path.join(path_checkpoint, f'overfeat_fast_states_epoch{epoch}.pkl')\n",
    "    else:\n",
    "        checkpoint_path = os.path.join(path_checkpoint, f'overfeat_accurate_states_epoch{epoch}.pkl')\n",
    "    state = {\n",
    "        'epoch': epoch,\n",
    "        'step': step,\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'model': overfeat.state_dict(),\n",
    "        'seed' : SEED\n",
    "    }\n",
    "    torch.save(state, checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End......"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
